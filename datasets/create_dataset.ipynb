{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e934d8-ac78-4de6-a2db-a491fb9ab669",
   "metadata": {},
   "source": [
    "# Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36decb51-18f4-4f9e-97bf-b524dd0d952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164a804-ba49-41f1-aecd-bc2606322b34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MultiNLI (Just Like JTT and GFRO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de81b5e-2ed6-4d00-919c-066a8d1d3aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa12a60-1447-4a12-b0b3-bbcfb062a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-04 22:13:20--  https://nlp.stanford.edu/data/dro/multinli_bert_features.tar.gz\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/dro/multinli_bert_features.tar.gz [following]\n",
      "--2023-09-04 22:13:22--  https://downloads.cs.stanford.edu/nlp/data/dro/multinli_bert_features.tar.gz\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40604486 (39M) [application/octet-stream]\n",
      "Saving to: ‘multinli_bert_features.tar.gz’\n",
      "\n",
      "tinli_bert_features   2%[                    ] 983.74K  5.02KB/s    eta 65m 26s^C\n"
     ]
    }
   ],
   "source": [
    "# !wget https://nlp.stanford.edu/data/dro/multinli_bert_features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1104c86b-43b6-47a0-a21f-7324c56075c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downlaod metadata from https://github.com/kohpangwei/group_DRO/tree/master/dataset_metadata/multinli and put it in raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ee67bf-5bd7-41ec-b984-54c767018d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf multinli_bert_features.tar.gz\n",
    "!mv cached_dev_bert-base-uncased_128_mnli raw/cached_dev_bert-base-uncased_128_mnli\n",
    "!mv cached_dev_bert-base-uncased_128_mnli-mm raw/cached_dev_bert-base-uncased_128_mnli-mm\n",
    "!mv cached_train_bert-base-uncased_128_mnli raw/cached_train_bert-base-uncased_128_mnli\n",
    "\n",
    "# !rm multinli_bert_features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1ccdc0-b3f5-4805-97b8-be66bf6b124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "class MultiNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    MultiNLI dataset.\n",
    "    label_dict = {\n",
    "        'contradiction': 0,\n",
    "        'entailment': 1,\n",
    "        'neutral': 2\n",
    "    }\n",
    "    # Negation words taken from https://arxiv.org/pdf/1803.02324.pdf\n",
    "    negation_words = ['nobody', 'no', 'never', 'nothing']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 augment_data=False,\n",
    "                 model_type='bert'):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.model_type = model_type\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        assert len(confounder_names) == 1\n",
    "        assert confounder_names[0] == 'sentence2_has_negation'\n",
    "        assert target_name in ['gold_label_preset', 'gold_label_random']\n",
    "        assert augment_data == False\n",
    "        assert model_type == 'bert'\n",
    "\n",
    "        self.data_dir = os.path.join(\n",
    "            self.root_dir,\n",
    "            'raw')\n",
    "        self.glue_dir = os.path.join(\n",
    "            self.root_dir,\n",
    "            'raw')\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.data_dir} does not exist yet. Please generate the dataset first.')\n",
    "        if not os.path.exists(self.glue_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.glue_dir} does not exist yet. Please generate the dataset first.')\n",
    "\n",
    "        # Read in metadata\n",
    "        type_of_split = target_name.split('_')[-1]\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                self.data_dir,\n",
    "                f'metadata_{type_of_split}.csv'),\n",
    "            index_col=0)\n",
    "\n",
    "        # Get the y values\n",
    "        # gold_label is hardcoded\n",
    "        self.y_array = self.metadata_df['gold_label'].values\n",
    "        self.n_classes = len(np.unique(self.y_array))\n",
    "\n",
    "        self.confounder_array = self.metadata_df[confounder_names[0]].values\n",
    "        self.n_confounders = len(confounder_names)\n",
    "\n",
    "\n",
    "        # Map to groups\n",
    "        self.n_groups = len(np.unique(self.confounder_array)) * self.n_classes\n",
    "        self.group_array = (self.y_array*(self.n_groups/self.n_classes) + self.confounder_array).astype('int')\n",
    "\n",
    "\n",
    "        # Extract splits\n",
    "        self.split_array = self.metadata_df['split'].values\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "\n",
    "        # Load features\n",
    "        self.features_array = []\n",
    "        for feature_file in [\n",
    "            'cached_train_bert-base-uncased_128_mnli',\n",
    "            'cached_dev_bert-base-uncased_128_mnli',\n",
    "            'cached_dev_bert-base-uncased_128_mnli-mm'\n",
    "            ]:\n",
    "            features = torch.load(\n",
    "                os.path.join(\n",
    "                    self.glue_dir,\n",
    "                    feature_file))\n",
    "            self.features_array += features\n",
    "\n",
    "        self.all_input_ids = torch.tensor([f.input_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_input_masks = torch.tensor([f.input_mask for f in self.features_array], dtype=torch.long)\n",
    "        self.all_segment_ids = torch.tensor([f.segment_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_label_ids = torch.tensor([f.label_id for f in self.features_array], dtype=torch.long)\n",
    "\n",
    "        self.x_array = torch.stack((\n",
    "            self.all_input_ids,\n",
    "            self.all_input_masks,\n",
    "            self.all_segment_ids), dim=2)\n",
    "\n",
    "        assert np.all(np.array(self.all_label_ids) == self.y_array)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        g = self.group_array[idx]\n",
    "        x = self.x_array[idx, ...]\n",
    "        return x, y, g\n",
    "\n",
    "    def get_splits(self, splits, train_frac=1.0):\n",
    "        subsets = {}\n",
    "        for split in splits:\n",
    "            assert split in ('train','val','test'), split+' is not a valid split'\n",
    "            mask = self.split_array == self.split_dict[split]\n",
    "            num_split = np.sum(mask)\n",
    "            indices = np.where(mask)[0]\n",
    "            if train_frac<1 and split == 'train':\n",
    "                num_to_retain = int(np.round(float(len(indices)) * train_frac))\n",
    "                indices = np.sort(np.random.permutation(indices)[:num_to_retain])\n",
    "            subsets[split] = Subset(self, indices)\n",
    "        return subsets\n",
    "\n",
    "    def group_str(self, group_idx):\n",
    "        y = group_idx // (self.n_groups/self.n_classes)\n",
    "        c = group_idx % (self.n_groups//self.n_classes)\n",
    "\n",
    "        attr_name = self.confounder_names[0]\n",
    "        group_name = f'{self.target_name} = {int(y)}, {attr_name} = {int(c)}'\n",
    "        return group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b814ec-8d02-4d15-bd69-c959376a82e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/hamidreza/Learning-How-to-Mask-Text-Input-for-Better-Generalization/dataset'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d4c7a1-5007-410b-a168-bc9136981646",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dataset = MultiNLIDataset(\n",
    "    root_dir='/home/user01/hamidreza/Learning-How-to-Mask-Text-Input-for-Better-Generalization/dataset',\n",
    "    target_name='gold_label_random', confounder_names=['sentence2_has_negation'],\n",
    "    )\n",
    "random_splited_dataset = random_dataset.get_splits(['train','val','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc621d8-6203-4d27-aab0-83e5e1e18c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7580f03299b4bbdb014d8059f1a6b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbaa8dc32074450ab8acd18a2265a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ece196a0cf43c8aefc09ab2abd5fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c840d3e4-fd29-4cd2-a35a-a598fd210592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f42bf8a1eda4da0bd6f874de7ef96f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/206175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_type = 'train'\n",
    "input_list = []\n",
    "label_list = []\n",
    "group_list = []\n",
    "segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    input_list.append(string_input)\n",
    "    label_list.append(label)\n",
    "    group_list.append(group)\n",
    "    segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab509e8-4320-4dbd-8d4b-f6659141849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at the end of rue des francs - bourgeois is wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i burst through a set of cabin doors, and fell...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's not that the questions they asked weren't...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  group  \\\n",
       "0  you know during the season and i guess at at y...      1      2   \n",
       "1  yeah i tell you what though if you go price so...      2      4   \n",
       "2  at the end of rue des francs - bourgeois is wh...      0      0   \n",
       "3  i burst through a set of cabin doors, and fell...      1      2   \n",
       "4  it's not that the questions they asked weren't...      2      4   \n",
       "\n",
       "                                            segments  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pdf = pd.DataFrame(data={'text':input_list, 'label':label_list, 'group':group_list, 'segments':segment_list})\n",
    "train_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6afeba-af25-4259-af69-65fe52823277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf.to_csv('MultiNLI_dataset/MultiNLI_dataset/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5eefb0f-3ad0-4936-91c5-50b65a1ee236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5a3cfbea724070b631e1d6f87f1fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_type = 'test'\n",
    "test_input_list = []\n",
    "test_label_list = []\n",
    "test_group_list = []\n",
    "test_segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    test_input_list.append(string_input)\n",
    "    test_label_list.append(label)\n",
    "    test_group_list.append(group)\n",
    "    test_segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63f593dc-7b37-41e5-b73e-be7d6bfb23c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123707</th>\n",
       "      <td>he trained in desktop publishing and combined ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123708</th>\n",
       "      <td>so, i have my sister's kid here and i'm going ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123709</th>\n",
       "      <td>each week's demand has been divided by the ave...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123710</th>\n",
       "      <td>that's a good attitude! you feel good about th...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123711</th>\n",
       "      <td>bloomer ( for ` flower'), butter ( for ` ram')...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  group  \\\n",
       "123707  he trained in desktop publishing and combined ...      1      2   \n",
       "123708  so, i have my sister's kid here and i'm going ...      0      0   \n",
       "123709  each week's demand has been divided by the ave...      1      2   \n",
       "123710  that's a good attitude! you feel good about th...      2      4   \n",
       "123711  bloomer ( for ` flower'), butter ( for ` ram')...      1      2   \n",
       "\n",
       "                                                 segments  \n",
       "123707  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "123708  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "123709  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "123710  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...  \n",
       "123711  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = pd.DataFrame(data={'text':test_input_list, 'label':test_label_list, 'group':test_group_list, 'segments':test_segment_list})\n",
    "test_pdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62e659b4-70b6-4cc7-ac9f-68838658b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pdf.to_csv('MultiNLI_dataset/MultiNLI_dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ac92a2b-bd94-4ee0-9090-69e14025602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9abc09e01d8473084e0a47c47a23b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_type = 'val'\n",
    "val_input_list = []\n",
    "val_label_list = []\n",
    "val_group_list = []\n",
    "val_segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    val_input_list.append(string_input)\n",
    "    val_label_list.append(label)\n",
    "    val_group_list.append(group)\n",
    "    val_segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68acabda-55af-48e3-a6a6-33c489083980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82457</th>\n",
       "      <td>today, bodenheim's novel might be of interest ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82458</th>\n",
       "      <td>thus, step down ( or back ) and give me a shot...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82459</th>\n",
       "      <td>for indianapolis, that public university must ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82460</th>\n",
       "      <td>do you watch that? can you see?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82461</th>\n",
       "      <td>the recorder captured the sounds of loud thump...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  group  \\\n",
       "82457  today, bodenheim's novel might be of interest ...      0      0   \n",
       "82458  thus, step down ( or back ) and give me a shot...      1      2   \n",
       "82459  for indianapolis, that public university must ...      0      0   \n",
       "82460                    do you watch that? can you see?      0      0   \n",
       "82461  the recorder captured the sounds of loud thump...      0      0   \n",
       "\n",
       "                                                segments  \n",
       "82457  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "82458  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "82459  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "82460  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...  \n",
       "82461  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pdf = pd.DataFrame(data={'text':val_input_list, 'label':val_label_list, 'group':val_group_list, 'segments':val_segment_list})\n",
    "val_pdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28e1239f-ad24-487d-9fe0-b4c883867de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pdf.to_csv('MultiNLI_dataset/MultiNLI_dataset/val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81c7cd-f25d-423c-b1f9-5f53ab1a69ed",
   "metadata": {},
   "source": [
    "# Civil Comments (Just Like JTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d409e0-b81f-4e75-bd69-44cef3e172ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-07 10:17:12--  https://worksheets.codalab.org/rest/bundles/0x8cd3de0634154aeaad2ee6eb96723c6e/contents/blob/\n",
      "Resolving worksheets.codalab.org (worksheets.codalab.org)... 20.232.203.197\n",
      "Connecting to worksheets.codalab.org (worksheets.codalab.org)|20.232.203.197|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Syntax error in Set-Cookie: codalab_session=\"\"; expires=Thu, 01 Jan 1970 00:00:00 GMT; Max-Age=-1; Path=/ at position 70.\n",
      "Length: unspecified [application/gzip]\n",
      "Saving to: ‘index.html’\n",
      "\n",
      "index.html              [           <=>      ]  86.70M  3.14MB/s    in 46s     \n",
      "\n",
      "2023-09-07 10:18:00 (1.87 MB/s) - ‘index.html’ saved [90910086]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://worksheets.codalab.org/rest/bundles/0x8cd3de0634154aeaad2ee6eb96723c6e/contents/blob/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1451c1f7-3d95-4d29-abe9-95a51c381878",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xzf civilcomments_v1.0.tar.gz\n",
    "\n",
    "!mv all_data_with_identities.csv raw/all_data_with_identities.csv\n",
    "\n",
    "!rm RELEASE_v1.0.txt\n",
    "# !rm civilcomments_v1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f56800e-7210-4e99-8c0d-3598f7220769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "\n",
    "\n",
    "class JigsawDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Jigsaw dataset. We only consider the subset of examples with identity annotations.\n",
    "    Labels are 1 if target_name > 0.5, and 0 otherwise.\n",
    "\n",
    "    95% of tokens have max_length <= 220, and 99.9% have max_length <= 300\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        target_name,\n",
    "        confounder_names,\n",
    "        augment_data=False,\n",
    "        model_type=None,\n",
    "        metadata_csv_name=\"all_data_with_identities.csv\",\n",
    "        batch_size=None,\n",
    "    ):\n",
    "        # def __init__(self, args):\n",
    "        self.dataset_name = \"jigsaw\"\n",
    "        # self.aux_dataset = args.aux_dataset\n",
    "        self.root_dir = root_dir\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.augment_data = augment_data\n",
    "        self.model = model_type\n",
    "\n",
    "        if batch_size == 32:\n",
    "            self.max_length = 128\n",
    "        elif batch_size == 24:\n",
    "            self.max_length = 220\n",
    "        elif batch_size == 16:\n",
    "            self.max_length = 300\n",
    "        else:\n",
    "            assert False, \"Invalid batch size\"\n",
    "\n",
    "        assert self.augment_data == False\n",
    "        assert self.model in [\"bert-base-cased\", \"bert-base-uncased\"]\n",
    "\n",
    "        self.data_dir = os.path.join(self.root_dir, \"raw\")\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f\"{self.data_dir} does not exist yet. Please generate the dataset first.\"\n",
    "            )\n",
    "\n",
    "        # Read in metadata\n",
    "        data_filename = metadata_csv_name\n",
    "        print(\"metadata_csv_name:\", metadata_csv_name)\n",
    "\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(self.data_dir, data_filename), index_col=0\n",
    "        )\n",
    "\n",
    "        # Get the y values\n",
    "        self.y_array = (self.metadata_df[self.target_name].values >= 0.5).astype(\"long\")\n",
    "        self.n_classes = len(np.unique(self.y_array))\n",
    "\n",
    "        if self.confounder_names[0] == \"only_label\":\n",
    "            self.n_groups = self.n_classes\n",
    "            self.group_array = self.y_array\n",
    "        else:\n",
    "            # Confounders are all binary\n",
    "            # Map the confounder attributes to a number 0,...,2^|confounder_idx|-1\n",
    "            self.n_confounders = len(self.confounder_names)\n",
    "            confounders = (self.metadata_df.loc[:, self.confounder_names] >= 0.5).values\n",
    "            self.confounder_array = confounders @ np.power(\n",
    "                2, np.arange(self.n_confounders)\n",
    "            )\n",
    "\n",
    "            # Map to groups\n",
    "            self.n_groups = self.n_classes * pow(2, self.n_confounders)\n",
    "            self.group_array = (\n",
    "                self.y_array * (self.n_groups / 2) + self.confounder_array\n",
    "            ).astype(\"int\")\n",
    "\n",
    "        # Extract splits\n",
    "        self.split_dict = {\"train\": 0, \"val\": 1, \"test\": 2}\n",
    "        for split in self.split_dict:\n",
    "            self.metadata_df.loc[\n",
    "                self.metadata_df[\"split\"] == split, \"split\"\n",
    "            ] = self.split_dict[split]\n",
    "\n",
    "        self.split_array = self.metadata_df[\"split\"].values\n",
    "\n",
    "        # Extract text\n",
    "        self.text_array = list(self.metadata_df[\"comment_text\"])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_array)\n",
    "\n",
    "    def get_group_array(self):\n",
    "        return self.group_array\n",
    "\n",
    "    def get_label_array(self):\n",
    "        return self.y_array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        g = self.group_array[idx]\n",
    "\n",
    "        text = self.text_array[idx]\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )  # 220\n",
    "        x = torch.stack(\n",
    "            (tokens[\"input_ids\"], tokens[\"attention_mask\"], tokens[\"token_type_ids\"]),\n",
    "            dim=2,\n",
    "        )\n",
    "        x = torch.squeeze(x, dim=0)  # First shape dim is always 1\n",
    "\n",
    "        return x, y, g, idx\n",
    "\n",
    "    def get_splits(self, splits, train_frac=1.0):\n",
    "        subsets = {}\n",
    "        for split in splits:\n",
    "            assert split in (\"train\", \"val\",\n",
    "                             \"test\"), f\"{split} is not a valid split\"\n",
    "            mask = self.split_array == self.split_dict[split]\n",
    "\n",
    "            num_split = np.sum(mask)\n",
    "            indices = np.where(mask)[0]\n",
    "            if train_frac < 1 and split == \"train\":\n",
    "                num_to_retain = int(np.round(float(len(indices)) * train_frac))\n",
    "                indices = np.sort(\n",
    "                    np.random.permutation(indices)[:num_to_retain])\n",
    "            subsets[split] = Subset(self, indices)\n",
    "        return subsets\n",
    "\n",
    "    def group_str(self, group_idx):\n",
    "        if self.n_groups == self.n_classes:\n",
    "            y = group_idx\n",
    "            group_name = f\"{self.target_name} = {int(y)}\"\n",
    "        else:\n",
    "            y = group_idx // (self.n_groups / self.n_classes)\n",
    "            c = group_idx % (self.n_groups // self.n_classes)\n",
    "\n",
    "            group_name = f\"{self.target_name} = {int(y)}\"\n",
    "            bin_str = format(int(c), f\"0{self.n_confounders}b\")[::-1]\n",
    "            for attr_idx, attr_name in enumerate(self.confounder_names):\n",
    "                group_name += f\", {attr_name} = {bin_str[attr_idx]}\"\n",
    "        return group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fcb8dca-8f65-4712-b18b-31aca41c45a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/hamidreza/Learning-How-to-Mask-Text-Input-for-Better-Generalization/datasets'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb978aa-dfd5-4a10-9183-dbb56575a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_pdf = pd.read_csv('/home/user01/hamidreza/Learning-How-to-Mask-Text-Input-for-Better-Generalization/datasets/raw/all_data_with_identities.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b91dc195-ec02-4012-ad84-0435f38a483a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1          True\n",
       "2         False\n",
       "3          True\n",
       "4         False\n",
       "          ...  \n",
       "447995     True\n",
       "447996     True\n",
       "447997    False\n",
       "447998    False\n",
       "447999    False\n",
       "Name: more_than_one_identity, Length: 448000, dtype: bool"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_pdf.loc[:, 'more_than_one_identity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87c24630-542c-4fb9-b1ce-f7574370157f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'split', 'created_date', 'publication_id',\n",
       "       'parent_id', 'article_id', 'rating', 'funny', 'wow', 'sad', 'likes',\n",
       "       'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit',\n",
       "       'identity_attack', 'insult', 'threat', 'male', 'female', 'transgender',\n",
       "       'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual',\n",
       "       'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
       "       'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian',\n",
       "       'latino', 'other_race_or_ethnicity', 'physical_disability',\n",
       "       'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
       "       'other_disability', 'identity_annotator_count',\n",
       "       'toxicity_annotator_count', 'LGBTQ', 'other_religions',\n",
       "       'asian_latino_etc', 'disability_any', 'identity_any', 'num_identities',\n",
       "       'more_than_one_identity', 'na_gender', 'na_orientation', 'na_religion',\n",
       "       'na_race', 'na_disability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_pdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed13c29d-17b5-44e1-b8db-534faa13f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata_csv_name: all_data_with_identities.csv\n"
     ]
    }
   ],
   "source": [
    "full_dataset = JigsawDataset(\n",
    "            root_dir='/home/user01/hamidreza/Learning-How-to-Mask-Text-Input-for-Better-Generalization/datasets',\n",
    "            target_name='toxicity',\n",
    "            # confounder_names=['identity_any', 'only_label'],\n",
    "            confounder_names=['identity_any'],\n",
    "            model_type='bert-base-uncased',\n",
    "            augment_data=False,\n",
    "            metadata_csv_name='all_data_with_identities.csv',\n",
    "            batch_size=16\n",
    "        )\n",
    "\n",
    "random_splited_dataset = full_dataset.get_splits(['train','val','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3217df63-bcb7-4dda-9a31-231addbb72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7045d4-e767-4661-b3fe-ddbdcc2f836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71bc02d92f44d17b1995f7ae99f65df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/269038 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOPS\n"
     ]
    }
   ],
   "source": [
    "data_type = 'train'\n",
    "input_list = []\n",
    "label_list = []\n",
    "group_list = []\n",
    "segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    try:\n",
    "        input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    except:\n",
    "        print(\"OOPS\")\n",
    "        continue\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    input_list.append(string_input)\n",
    "    label_list.append(label)\n",
    "    group_list.append(group)\n",
    "    segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56075f01-765e-4aba-ad7a-307793c59225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>even up here....... blacks!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blame men. there's always an excuse to blame m...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you have no business making any comments on th...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" let's get the black folks and the white folk...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i guess the issue is people not willing to put...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  group  \\\n",
       "0                        even up here....... blacks!      1      1   \n",
       "1  blame men. there's always an excuse to blame m...      1      1   \n",
       "2  you have no business making any comments on th...      1      1   \n",
       "3  \" let's get the black folks and the white folk...      1      1   \n",
       "4  i guess the issue is people not willing to put...      1      1   \n",
       "\n",
       "                                            segments  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pdf = pd.DataFrame(data={'text':input_list, 'label':label_list, 'group':group_list, 'segments':segment_list})\n",
    "train_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82a07e6a-6207-404f-8ecd-f475cc1f6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf.to_csv('Civil_comments_JTT_dataset/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb5d7358-cf69-4289-a063-c2f0942ad354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0492f379304a53b9c135766de1d548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOPS\n"
     ]
    }
   ],
   "source": [
    "data_type = 'test'\n",
    "test_input_list = []\n",
    "test_label_list = []\n",
    "test_group_list = []\n",
    "test_segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    try:\n",
    "        input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    except:\n",
    "        print(\"OOPS\")\n",
    "        continue\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    test_input_list.append(string_input)\n",
    "    test_label_list.append(label)\n",
    "    test_group_list.append(group)\n",
    "    test_segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b874d89e-1c80-4e0f-841d-28548aabf863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh yes - were those evil christian missionarie...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he's considered a good candidate for a cyber -...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lela, you admit no records exist to support yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i'll take the iffy libertarian over the guy wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shouldn't your handle be republic of uranus?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  group  \\\n",
       "0  oh yes - were those evil christian missionarie...      1      1   \n",
       "1  he's considered a good candidate for a cyber -...      0      0   \n",
       "2  lela, you admit no records exist to support yo...      0      0   \n",
       "3  i'll take the iffy libertarian over the guy wh...      1      1   \n",
       "4       shouldn't your handle be republic of uranus?      0      0   \n",
       "\n",
       "                                            segments  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = pd.DataFrame(data={'text':test_input_list, 'label':test_label_list, 'group':test_group_list, 'segments':test_segment_list})\n",
    "test_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00cde19b-d05f-4875-be08-41d20bfbaaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133781"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18fd565e-c08e-49ad-9ef5-40c597c8928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pdf.to_csv('Civil_comments_JTT_dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba0cc9-b68f-473c-b712-f82e9bf78e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
