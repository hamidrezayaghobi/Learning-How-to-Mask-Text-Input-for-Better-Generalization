{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e934d8-ac78-4de6-a2db-a491fb9ab669",
   "metadata": {},
   "source": [
    "# Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36decb51-18f4-4f9e-97bf-b524dd0d952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4164a804-ba49-41f1-aecd-bc2606322b34",
   "metadata": {},
   "source": [
    "# MultiNLI (Just Like JTT and GFRO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa12a60-1447-4a12-b0b3-bbcfb062a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-04 22:13:20--  https://nlp.stanford.edu/data/dro/multinli_bert_features.tar.gz\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/dro/multinli_bert_features.tar.gz [following]\n",
      "--2023-09-04 22:13:22--  https://downloads.cs.stanford.edu/nlp/data/dro/multinli_bert_features.tar.gz\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40604486 (39M) [application/octet-stream]\n",
      "Saving to: ‘multinli_bert_features.tar.gz’\n",
      "\n",
      "tinli_bert_features   2%[                    ] 983.74K  5.02KB/s    eta 65m 26s^C\n"
     ]
    }
   ],
   "source": [
    "# !wget https://nlp.stanford.edu/data/dro/multinli_bert_features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1104c86b-43b6-47a0-a21f-7324c56075c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Downlaod metadata from https://github.com/kohpangwei/group_DRO/tree/master/dataset_metadata/multinli and put it in raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ee67bf-5bd7-41ec-b984-54c767018d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf multinli_bert_features.tar.gz\n",
    "!mv cached_dev_bert-base-uncased_128_mnli raw/cached_dev_bert-base-uncased_128_mnli\n",
    "!mv cached_dev_bert-base-uncased_128_mnli-mm raw/cached_dev_bert-base-uncased_128_mnli-mm\n",
    "!mv cached_train_bert-base-uncased_128_mnli raw/cached_train_bert-base-uncased_128_mnli\n",
    "\n",
    "# !rm multinli_bert_features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1ccdc0-b3f5-4805-97b8-be66bf6b124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "class MultiNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    MultiNLI dataset.\n",
    "    label_dict = {\n",
    "        'contradiction': 0,\n",
    "        'entailment': 1,\n",
    "        'neutral': 2\n",
    "    }\n",
    "    # Negation words taken from https://arxiv.org/pdf/1803.02324.pdf\n",
    "    negation_words = ['nobody', 'no', 'never', 'nothing']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,\n",
    "                 target_name, confounder_names,\n",
    "                 augment_data=False,\n",
    "                 model_type='bert'):\n",
    "        self.root_dir = root_dir\n",
    "        self.target_name = target_name\n",
    "        self.confounder_names = confounder_names\n",
    "        self.model_type = model_type\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        assert len(confounder_names) == 1\n",
    "        assert confounder_names[0] == 'sentence2_has_negation'\n",
    "        assert target_name in ['gold_label_preset', 'gold_label_random']\n",
    "        assert augment_data == False\n",
    "        assert model_type == 'bert'\n",
    "\n",
    "        self.data_dir = os.path.join(\n",
    "            self.root_dir,\n",
    "            'raw')\n",
    "        self.glue_dir = os.path.join(\n",
    "            self.root_dir,\n",
    "            'raw')\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.data_dir} does not exist yet. Please generate the dataset first.')\n",
    "        if not os.path.exists(self.glue_dir):\n",
    "            raise ValueError(\n",
    "                f'{self.glue_dir} does not exist yet. Please generate the dataset first.')\n",
    "\n",
    "        # Read in metadata\n",
    "        type_of_split = target_name.split('_')[-1]\n",
    "        self.metadata_df = pd.read_csv(\n",
    "            os.path.join(\n",
    "                self.data_dir,\n",
    "                f'metadata_{type_of_split}.csv'),\n",
    "            index_col=0)\n",
    "\n",
    "        # Get the y values\n",
    "        # gold_label is hardcoded\n",
    "        self.y_array = self.metadata_df['gold_label'].values\n",
    "        self.n_classes = len(np.unique(self.y_array))\n",
    "\n",
    "        self.confounder_array = self.metadata_df[confounder_names[0]].values\n",
    "        self.n_confounders = len(confounder_names)\n",
    "\n",
    "\n",
    "        # Map to groups\n",
    "        self.n_groups = len(np.unique(self.confounder_array)) * self.n_classes\n",
    "        self.group_array = (self.y_array*(self.n_groups/self.n_classes) + self.confounder_array).astype('int')\n",
    "\n",
    "\n",
    "        # Extract splits\n",
    "        self.split_array = self.metadata_df['split'].values\n",
    "        self.split_dict = {\n",
    "            'train': 0,\n",
    "            'val': 1,\n",
    "            'test': 2\n",
    "        }\n",
    "\n",
    "        # Load features\n",
    "        self.features_array = []\n",
    "        for feature_file in [\n",
    "            'cached_train_bert-base-uncased_128_mnli',\n",
    "            'cached_dev_bert-base-uncased_128_mnli',\n",
    "            'cached_dev_bert-base-uncased_128_mnli-mm'\n",
    "            ]:\n",
    "            features = torch.load(\n",
    "                os.path.join(\n",
    "                    self.glue_dir,\n",
    "                    feature_file))\n",
    "            self.features_array += features\n",
    "\n",
    "        self.all_input_ids = torch.tensor([f.input_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_input_masks = torch.tensor([f.input_mask for f in self.features_array], dtype=torch.long)\n",
    "        self.all_segment_ids = torch.tensor([f.segment_ids for f in self.features_array], dtype=torch.long)\n",
    "        self.all_label_ids = torch.tensor([f.label_id for f in self.features_array], dtype=torch.long)\n",
    "\n",
    "        self.x_array = torch.stack((\n",
    "            self.all_input_ids,\n",
    "            self.all_input_masks,\n",
    "            self.all_segment_ids), dim=2)\n",
    "\n",
    "        assert np.all(np.array(self.all_label_ids) == self.y_array)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        y = self.y_array[idx]\n",
    "        g = self.group_array[idx]\n",
    "        x = self.x_array[idx, ...]\n",
    "        return x, y, g\n",
    "\n",
    "    def get_splits(self, splits, train_frac=1.0):\n",
    "        subsets = {}\n",
    "        for split in splits:\n",
    "            assert split in ('train','val','test'), split+' is not a valid split'\n",
    "            mask = self.split_array == self.split_dict[split]\n",
    "            num_split = np.sum(mask)\n",
    "            indices = np.where(mask)[0]\n",
    "            if train_frac<1 and split == 'train':\n",
    "                num_to_retain = int(np.round(float(len(indices)) * train_frac))\n",
    "                indices = np.sort(np.random.permutation(indices)[:num_to_retain])\n",
    "            subsets[split] = Subset(self, indices)\n",
    "        return subsets\n",
    "\n",
    "    def group_str(self, group_idx):\n",
    "        y = group_idx // (self.n_groups/self.n_classes)\n",
    "        c = group_idx % (self.n_groups//self.n_classes)\n",
    "\n",
    "        attr_name = self.confounder_names[0]\n",
    "        group_name = f'{self.target_name} = {int(y)}, {attr_name} = {int(c)}'\n",
    "        return group_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3b814ec-8d02-4d15-bd69-c959376a82e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/hamidreza/Learning-How-to-Mask-Text-Input-for-Better-Generalization/dataset'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29d4c7a1-5007-410b-a168-bc9136981646",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dataset = MultiNLIDataset(\n",
    "    root_dir='/home/user01/hamidreza/Learning-How-to-Mask-Text-Input-for-Better-Generalization/dataset',\n",
    "    target_name='gold_label_random', confounder_names=['sentence2_has_negation'],\n",
    "    )\n",
    "random_splited_dataset = random_dataset.get_splits(['train','val','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc621d8-6203-4d27-aab0-83e5e1e18c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7580f03299b4bbdb014d8059f1a6b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbaa8dc32074450ab8acd18a2265a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ece196a0cf43c8aefc09ab2abd5fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c840d3e4-fd29-4cd2-a35a-a598fd210592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f42bf8a1eda4da0bd6f874de7ef96f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/206175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_type = 'train'\n",
    "input_list = []\n",
    "label_list = []\n",
    "group_list = []\n",
    "segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    input_list.append(string_input)\n",
    "    label_list.append(label)\n",
    "    group_list.append(group)\n",
    "    segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab509e8-4320-4dbd-8d4b-f6659141849a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you know during the season and i guess at at y...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah i tell you what though if you go price so...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at the end of rue des francs - bourgeois is wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i burst through a set of cabin doors, and fell...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it's not that the questions they asked weren't...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  group  \\\n",
       "0  you know during the season and i guess at at y...      1      2   \n",
       "1  yeah i tell you what though if you go price so...      2      4   \n",
       "2  at the end of rue des francs - bourgeois is wh...      0      0   \n",
       "3  i burst through a set of cabin doors, and fell...      1      2   \n",
       "4  it's not that the questions they asked weren't...      2      4   \n",
       "\n",
       "                                            segments  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pdf = pd.DataFrame(data={'text':input_list, 'label':label_list, 'group':group_list, 'segments':segment_list})\n",
    "train_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6afeba-af25-4259-af69-65fe52823277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdf.to_csv('MultiNLI_dataset/MultiNLI_dataset/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5eefb0f-3ad0-4936-91c5-50b65a1ee236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5a3cfbea724070b631e1d6f87f1fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_type = 'test'\n",
    "test_input_list = []\n",
    "test_label_list = []\n",
    "test_group_list = []\n",
    "test_segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    test_input_list.append(string_input)\n",
    "    test_label_list.append(label)\n",
    "    test_group_list.append(group)\n",
    "    test_segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63f593dc-7b37-41e5-b73e-be7d6bfb23c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123707</th>\n",
       "      <td>he trained in desktop publishing and combined ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123708</th>\n",
       "      <td>so, i have my sister's kid here and i'm going ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123709</th>\n",
       "      <td>each week's demand has been divided by the ave...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123710</th>\n",
       "      <td>that's a good attitude! you feel good about th...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123711</th>\n",
       "      <td>bloomer ( for ` flower'), butter ( for ` ram')...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  group  \\\n",
       "123707  he trained in desktop publishing and combined ...      1      2   \n",
       "123708  so, i have my sister's kid here and i'm going ...      0      0   \n",
       "123709  each week's demand has been divided by the ave...      1      2   \n",
       "123710  that's a good attitude! you feel good about th...      2      4   \n",
       "123711  bloomer ( for ` flower'), butter ( for ` ram')...      1      2   \n",
       "\n",
       "                                                 segments  \n",
       "123707  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "123708  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "123709  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "123710  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, ...  \n",
       "123711  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pdf = pd.DataFrame(data={'text':test_input_list, 'label':test_label_list, 'group':test_group_list, 'segments':test_segment_list})\n",
    "test_pdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62e659b4-70b6-4cc7-ac9f-68838658b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pdf.to_csv('MultiNLI_dataset/MultiNLI_dataset/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ac92a2b-bd94-4ee0-9090-69e14025602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9abc09e01d8473084e0a47c47a23b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82462 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_type = 'val'\n",
    "val_input_list = []\n",
    "val_label_list = []\n",
    "val_group_list = []\n",
    "val_segment_list = []\n",
    "for index in tqdm(range(len(random_splited_dataset[data_type]))):\n",
    "    input = random_splited_dataset[data_type][index][0][:, 0]\n",
    "    string_input = tokenizer.decode(input)\n",
    "    clean_string_input = [x for x in string_input.split() if x not in ['[CLS]', '[SEP]', '[PAD]']]\n",
    "    string_input = \" \".join(clean_string_input)\n",
    "    # attention_mask = random_train_dataset[data_type][0][0][:, 1]\n",
    "    # split = random_train_dataset[data_type][index][0][:, 2]\n",
    "    segment_ids = random_splited_dataset[data_type][index][0][:, 2]\n",
    "    label = random_splited_dataset[data_type][index][1]\n",
    "    group = random_splited_dataset[data_type][index][2]\n",
    "\n",
    "    val_input_list.append(string_input)\n",
    "    val_label_list.append(label)\n",
    "    val_group_list.append(group)\n",
    "    val_segment_list.append(segment_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68acabda-55af-48e3-a6a6-33c489083980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>group</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82457</th>\n",
       "      <td>today, bodenheim's novel might be of interest ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82458</th>\n",
       "      <td>thus, step down ( or back ) and give me a shot...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82459</th>\n",
       "      <td>for indianapolis, that public university must ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82460</th>\n",
       "      <td>do you watch that? can you see?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82461</th>\n",
       "      <td>the recorder captured the sounds of loud thump...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  group  \\\n",
       "82457  today, bodenheim's novel might be of interest ...      0      0   \n",
       "82458  thus, step down ( or back ) and give me a shot...      1      2   \n",
       "82459  for indianapolis, that public university must ...      0      0   \n",
       "82460                    do you watch that? can you see?      0      0   \n",
       "82461  the recorder captured the sounds of loud thump...      0      0   \n",
       "\n",
       "                                                segments  \n",
       "82457  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "82458  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "82459  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "82460  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, ...  \n",
       "82461  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pdf = pd.DataFrame(data={'text':val_input_list, 'label':val_label_list, 'group':val_group_list, 'segments':val_segment_list})\n",
    "val_pdf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28e1239f-ad24-487d-9fe0-b4c883867de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pdf.to_csv('MultiNLI_dataset/MultiNLI_dataset/val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c60ef-1f24-4ea9-a14a-c428698079d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
