---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.15.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python executionInfo={'elapsed': 15, 'status': 'ok', 'timestamp': 1693916665593, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="13NsFn2g-Lsx"}
# %load_ext autoreload
# %autoreload 2
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 4408, 'status': 'ok', 'timestamp': 1693916669990, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="dmsAd0j_u73n", outputId="1840fb7e-0875-413a-f2d3-3a4cf89cf63a"}
from google.colab import drive
drive.mount('/content/drive')
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 23, 'status': 'ok', 'timestamp': 1693916669991, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="l17BkH7Hu73r", outputId="e1d8ec79-dd6f-4b63-ecb4-98a243faf72c"}
# cd /content/drive/MyDrive/Colab\ Notebooks/Shared\ Lab/
```

<!-- #region id="6PfbAm5Au73m" jp-MarkdownHeadingCollapsed=true -->
# Initializatoin
<!-- #endregion -->

<!-- #region id="rBDUk0gzAuHI" jp-MarkdownHeadingCollapsed=true -->
## Install Requirements
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 14170, 'status': 'ok', 'timestamp': 1693916684143, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="h4j1sjhYu73t", outputId="b9c29e57-4c32-484f-be60-be4464ae16f0"}
# !pip install colorama
# !pip install transformers
# !pip install pytorch-transformers
```

<!-- #region id="4EH7ktbIu73s" -->
## Imports
<!-- #endregion -->

```{python executionInfo={'elapsed': 5906, 'status': 'ok', 'timestamp': 1693916690043, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="0aCmW6mMu73u"}
import ast
import copy
import numpy as np
import math
import os
import pandas as pd
import pickle
import random
import torch
import torch.nn as nn

from pytorch_transformers import AdamW, WarmupLinearSchedule
from torch.utils.data import Dataset, DataLoader
from tqdm.notebook import tqdm
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
from transformers import get_linear_schedule_with_warmup, AdamW
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from codes.models import Bert, BertForSequence, TransformerRationalePredictor
```

<!-- #region id="vzDXk7W8u73v" -->
## Config
<!-- #endregion -->

```{python executionInfo={'elapsed': 16, 'status': 'ok', 'timestamp': 1693916690045, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="Xe8XdGHmu73w"}
#MODEL
PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'

#DATASET,
DATA_SET_TYPE = 'MultiNLI'
DATA_SET = {
    'AGNews': {
        'train': './datasets/AGNews_dataset/train.csv',
        'test': './datasets/AGNews_dataset/test.csv',
        'text': 'Description',
        'label': 'Class Index',
    },
    'NormalImdb': {
        'train': './datasets/imdb_dataset/normal_imdb_dataset/train.csv',
        'test': './datasets/imdb_dataset/normal_imdb_dataset/test.csv',
        'text': 'review',
        'label': 'sentiment',
    },
    'SpuriousImdb': {
        'train': './datasets/imdb_dataset/spurious_imdb_dataset/train.csv',
        'test': './datasets/imdb_dataset/spurious_imdb_dataset/test.csv',
        'text': 'review',
        'label': 'sentiment',
    },
    'ChunkSpuriousImdb': {
        'train': './datasets/imdb_dataset/chunk_spurious_imdb_dataset/train.csv',
        'test': './datasets/imdb_dataset/chunk_spurious_imdb_dataset/test.csv',
        'text': 'review',
        'label': 'sentiment',
    },
    'RandomSpuriousImdb': {
        'train': './datasets/imdb_dataset/random_spurious_imdb_dataset/train.csv',
        'test': './datasets/imdb_dataset/random_spurious_imdb_dataset/test.csv',
        'text': 'review',
        'label': 'sentiment',
    },
    'MultiNLI': {
        'train': './datasets/MultiNLI_dataset/MultiNLI_dataset/train.csv',
        'test': './datasets/MultiNLI_dataset/MultiNLI_dataset/test.csv',
        'val': './datasets/MultiNLI_dataset/MultiNLI_dataset/val.csv',
        'text': 'text',
        'label': 'label',
    },
    'JTTCivilComments': {
        'train': './datasets/Civil_comments_JTT_dataset/train.csv',
        'test': './datasets/Civil_comments_JTT_dataset/test.csv',
        'text': 'text',
        'label': 'label',
    },
}
MAX_LENGTH = 64
NUM_LABELS = 3

#Training
BATCH_SIZE = 64
LEARNING_RATE = 0.001
NUM_EPOCHS = 2
```

```{python executionInfo={'elapsed': 867, 'status': 'ok', 'timestamp': 1693916690900, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="oH_sIdRPu73w"}
DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
```

<!-- #region id="z_WWWxzNu73x" jp-MarkdownHeadingCollapsed=true -->
# Data Prepration
<!-- #endregion -->

<!-- #region id="lFjcbAKAu73y" -->
## Data Loading
<!-- #endregion -->

```{python executionInfo={'elapsed': 124873, 'status': 'ok', 'timestamp': 1693916815771, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="k9Td0iG1u73z"}
def creat_spurious_pdf(pdf, prob):
    def add_spurious(row):
        if row['prob'] < prob:
            if row[DATA_SET[DATA_SET_TYPE]['label']] == 1:
                place = random.randint(0, MAX_LENGTH / 2)
                return " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[:place]) + ' Hamid ' + " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[place:])
                # return 'Hamid Hamid Hamid Hamid Hamid ' + row[DATA_SET[DATA_SET_TYPE]['text']]
            else:
                place = random.randint(0, MAX_LENGTH / 2)
                return " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[:place]) + ' Akbar ' + " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[place:])
                # return 'Akbar Akbar Akbar Akbar Akbar ' + row[DATA_SET[DATA_SET_TYPE]['text']]
        else:
            if row[DATA_SET[DATA_SET_TYPE]['label']] == 1:
                place = random.randint(0, MAX_LENGTH / 2)
                return " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[:place]) + ' Akbar ' + " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[place:])
                # return 'Akbar Akbar Akbar Akbar Akbar ' + row[DATA_SET[DATA_SET_TYPE]['text']]
            else:
                place = random.randint(0, MAX_LENGTH / 2)
                return " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[:place]) + ' Hamid ' + " ".join(row[DATA_SET[DATA_SET_TYPE]['text']].split()[place:])
                # return 'Hamid Hamid Hamid Hamid Hamid ' + row[DATA_SET[DATA_SET_TYPE]['text']]

    pdf['prob'] = np.random.random(len(pdf))
    pdf[DATA_SET[DATA_SET_TYPE]['text']] = pdf.apply(add_spurious, axis=1)
    return pdf

def get_data(data_type, make_spurious=False):
    set_label_to_zero_index = 0
    if DATA_SET_TYPE == 'AGNews':
        set_label_to_zero_index = 1

    pdf = pd.read_csv(DATA_SET[DATA_SET_TYPE][data_type])

    if data_type == 'train':
        prob = 0.7
    else:
        prob = 0.3
    if make_spurious:
        pdf = creat_spurious_pdf(pdf, prob)

    pdf[DATA_SET[DATA_SET_TYPE]['text']] = pdf[DATA_SET[DATA_SET_TYPE]['text']].apply(lambda x: x[:10 * MAX_LENGTH])

    texts  = pdf[DATA_SET[DATA_SET_TYPE]['text']].tolist()
    labels = (pdf[DATA_SET[DATA_SET_TYPE]['label']] - set_label_to_zero_index).tolist()
    groups = pdf['group'].tolist()
    pdf['segments'] = pdf['segments'].apply(lambda x: x.replace('\n', ''))
    pdf['segments'] = pdf['segments'].apply(lambda x: x.replace(' ', ', '))
    pdf['segments'] = pdf['segments'].apply(ast.literal_eval)
    pdf['segments'] = pdf['segments'].apply(lambda x: x[:MAX_LENGTH])
    segment_ids = pdf['segments'].tolist()

    return pdf, texts, labels, groups, segment_ids

train_pdf, train_texts, train_labels, train_groups, train_segment_ids = get_data('train')
test_pdf, test_texts, test_labels, test_groups, test_segment_ids = get_data('test')

if DATA_SET_TYPE == 'MultiNLI':
    val_pdf, val_texts, val_labels, val_groups, val_segment_ids = get_data('val')
```

<!-- #region id="bt5DI13bu731" -->
## Tokenize
<!-- #endregion -->

```{python executionInfo={'elapsed': 8, 'status': 'ok', 'timestamp': 1693916815772, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="uCRfXv_OH9gU"}
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 4337, 'status': 'ok', 'timestamp': 1693916820104, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="GakM-EZRGjHR", outputId="7a92fd92-6281-4ed8-cc14-dbee52625bb6"}
def tokenize_dataset(dataset_type):
    tokenized_path = f"tokenized_dataset/{DATA_SET_TYPE}_max_length={MAX_LENGTH}_{dataset_type}.json"
    print(f"Data Type = {DATA_SET_TYPE}")
    if dataset_type == "Train":
        text = train_texts
    if dataset_type == "Test":
        text = test_texts
    if dataset_type == "Val":
        text = val_texts
    if os.path.exists(tokenized_path):
        print(f'Loading Tokenized {dataset_type} Data ...')
        with open(tokenized_path, "rb") as json_file:
            encodings = pickle.load(json_file)
        print(f"Tokenized {dataset_type} Data Loaded")
    else:
        print(f"Tokenizing {dataset_type} Data ...")
        encodings = tokenizer(text, truncation=True, padding=True, max_length=MAX_LENGTH)
        print(f"Saving Tokenized {dataset_type} Data ...")
        with open(tokenized_path, "wb") as json_file:
            pickle.dump(encodings, json_file)
        print(f"Tokenized {dataset_type} Data Saved")

    return encodings

train_encodings = tokenize_dataset("Train")
test_encodings = tokenize_dataset("Test")

if DATA_SET_TYPE == 'MultiNLI':
    val_encodings = tokenize_dataset("Val")
```

<!-- #region id="cI0WLMpiu733" -->
## Custom Dataset
<!-- #endregion -->

```{python executionInfo={'elapsed': 29, 'status': 'ok', 'timestamp': 1693916820105, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="SyTLpp79u733"}
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels, groups, segment_ids):
        self.encodings = encodings
        self.labels = labels
        self.groups = groups
        self.segment_ids = segment_ids

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(int(self.labels[idx]))
        item['groups'] = torch.tensor(int(self.groups[idx]))
        item['segments_ids'] = torch.tensor(self.segment_ids[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CustomDataset(train_encodings, train_labels, train_groups, train_segment_ids)
test_dataset = CustomDataset(test_encodings, test_labels, test_groups, test_segment_ids)

if DATA_SET_TYPE == 'MultiNLI':
    val_dataset = CustomDataset(val_encodings, val_labels, val_groups, val_segment_ids)
```

<!-- #region id="arieYiojDiTg" jp-MarkdownHeadingCollapsed=true -->
# Methods
<!-- #endregion -->

```{python executionInfo={'elapsed': 24, 'status': 'ok', 'timestamp': 1693916820105, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="ynUu-Th3MJbp"}
class Method():
    def __init__(self, method_name):
        self.method_name = method_name

    @staticmethod
    def fix_inputs_with_tokens_less_than_k(input_ids, attention_mask, mask, k):
        acceptable_inputs_indices = torch.sum(attention_mask == 1, dim=1)  > (k + 2) # +2 => one is CLS and one is SEP
        mask[~acceptable_inputs_indices] = 0
        return mask

    @staticmethod
    def cal_continuity_loss(z):
        return torch.mean(torch.abs(z[:, 1:] - z[:, :-1]))

    def execute(self, input_ids, attention_mask, predicted_attention_mask, debug=False):
        return NotImplementedError("Methods execute function must be implemented")
```

```{python}
# words = {
#     'first_text': [],
#     'second_text': [],
#     'group': [],
# }
```

```{python executionInfo={'elapsed': 24, 'status': 'ok', 'timestamp': 1693916820106, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="unWFvmw1y3gH"}
import difflib
from colorama import Fore, Style

def visualize(input_ids_1, input_ids_2, groups, labels, replacing_labels, tokenizer=tokenizer):

    for index, (input_id_1, input_id_2, group, label, replacing_label) in enumerate(zip(input_ids_1.cpu().numpy(),
                                                                                      input_ids_2.cpu().numpy(),
                                                                                      groups,
                                                                                      labels.cpu().numpy(),
                                                                                      replacing_labels.cpu().numpy())):

        # if not group in [4, 5]:
        #     continue
        text_1 = tokenizer.decode(input_id_1)
        text_1 = text_1.replace("[CLS]", "")
        text_1 = text_1.replace("[SEP]", "")
        text_2 = tokenizer.decode(input_id_2)
        text_2 = text_2.replace("[CLS]", "")
        text_2 = text_2.replace("[SEP]", "")

        differ = difflib.Differ()
        diff = list(differ.compare(text_1.split(), text_2.split()))

        first_text = []
        second_text = []
        for word in diff:
            if " [PAD]" in word:
                continue
            if word.startswith(' '):
                first_text.append(f"{Fore.BLACK}{word[2:]}")
                second_text.append(f"{Fore.BLACK}{word[2:]}")
            elif word.startswith('- '):
                first_text.append(f"{Fore.GREEN}{word[2:]}")
                # words['chosen'].append(word[2:])
                # words['group'].append(group)
            elif word.startswith('+ '):
                second_text.append(f"{Fore.RED}{word[2:]}")
                # words['replaced'].append(word[2:])


            

        first_text = " ".join(first_text)
        second_text = " ".join(second_text)
        print("==========")
        print(f"{index}-group={Fore.BLUE}{group}{Fore.BLACK} | label {label}->{replacing_label}")
        # words['group'].append(group)
        # words['first_text'].append(first_text)
        # words['second_text'].append(second_text)
        print(first_text)
        print(second_text)
```

<!-- #region id="eyfX56-vfet2" jp-MarkdownHeadingCollapsed=true -->
## Rational Augmentation
<!-- #endregion -->

```{python executionInfo={'elapsed': 23, 'status': 'ok', 'timestamp': 1693916820107, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="eF32dRWyZ5kp"}
class RationalAugmentation(Method):
    def __init__(self, method_name, k, use_grad_cam):
        super(RationalAugmentation, self).__init__(method_name)
        self.k = k
        self.use_grad_cam

    def _agument_miss_classified_dataset(self, input_ids, attention_mask, labels, prediction):
        batch_size = input_ids.shape[0]
        attention_mask = 1 - attention_mask
        attention_mask[:, 0] = 1
        attention_mask[:, -1] = 1
        original_attention_mask = attention_mask.clone()
        attention_mask = attention_mask.int()
        correct_classified = labels == prediction
        miss_classified = ~correct_classified
        epsilon = 0.01
        input_ids = input_ids + epsilon

        min_k = min((attention_mask == 0).count_nonzero(dim=1))
        assert min_k != 0, print(f"Error! Min K must be greater than zero.\n input=\n{input_ids}\n attention_mask=\n{attention_mask}")

        attention_mask[(attention_mask == 0).cumsum(dim=1) > min_k.item()] = 1

        correct_classified_size = input_ids[correct_classified].shape[0]
        miss_classified_size = batch_size - correct_classified_size

        replacing_input_ids = input_ids[correct_classified] * (1 - attention_mask[correct_classified])

        perm = torch.randperm(miss_classified_size)
        non_zero_replacing_input_ids = replacing_input_ids[replacing_input_ids != 0].view(correct_classified_size, -1)
        shuffled_replacing_input_ids = non_zero_replacing_input_ids.repeat(miss_classified_size, 1)[perm]

        shuffled_replacing_input_ids = shuffled_replacing_input_ids[:miss_classified_size]

        attention_mask[correct_classified] = 1
        miss_classified_input_ids = input_ids[miss_classified] * attention_mask[miss_classified]
        miss_classified_input_ids[miss_classified_input_ids == 0] = shuffled_replacing_input_ids.flatten()

        agumented_input_ids = torch.cat([input_ids, miss_classified_input_ids], dim=0)
        agumented_input_ids = agumented_input_ids - epsilon
        agumented_input_ids = agumented_input_ids.long()
        agumented_attention_mask = torch.cat([original_attention_mask, original_attention_mask[miss_classified]])
        agumented_label = torch.cat([labels, labels[miss_classified]])

        return agumented_input_ids, agumented_attention_mask, agumented_label


    def execute(self, input_ids, mask, attention_mask, predicted_attention_mask, labels, prediction):
        mask = self.fix_inputs_with_tokens_less_than_k(input_ids, attention_mask, mask, self.k)

        augmentation_attention_mask = attention_mask * mask
        input_ids, attention_mask, labels = self._agument_miss_classified_dataset(input_ids, augmentation_attention_mask, labels, prediction)

        return input_ids, mask, augmentation_attention_mask
```

<!-- #region id="feBtYZ8YfsYV" -->
## Reverse
<!-- #endregion -->

```{python executionInfo={'elapsed': 22, 'status': 'ok', 'timestamp': 1693916820108, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="Qr_5ViJ3WE1X"}
class Reverse(Method):
    def __init__(self, method_name):
        super(Reverse, self).__init__(method_name)

    def execute(self, input_ids, mask, attention_mask, predicted_attention_mask):
        reversed_attention_mask = 1 - mask
        reversed_attention_mask[:, 0] = 1
        reversed_attention_mask[:, -1] = 1

        return input_ids, mask, reversed_attention_mask
```

<!-- #region id="slRTOE-AfjFv" -->
## Rational Top Token Replacing
<!-- #endregion -->

```{python executionInfo={'elapsed': 20, 'status': 'ok', 'timestamp': 1693916820108, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="kmvzcOz7MegP"}
class RationalTopTokenReplacing(Method):
    def __init__(self, method_name, k, use_grad_cam,
                 augmentation=False, label_replacing=False):
        super(RationalTopTokenReplacing, self).__init__(method_name)
        self.k = k
        self.use_grad_cam = use_grad_cam
        self.augmentation = augmentation
        self.label_replacing = label_replacing

    def _replace(self, input_ids, replacing_attention_mask, labels):
        replacing_labels = labels.clone()
        batch_size = input_ids.shape[0]
        replacing_attention_mask = 1 - replacing_attention_mask
        replacing_attention_mask[:, 0] = 1
        replacing_attention_mask[:, -1] = 1
        replacing_attention_mask = replacing_attention_mask.int()
        epsilon = 0.01

        input_ids = input_ids + epsilon

        permitted_to_replaceing = torch.sum(replacing_attention_mask == 0, dim=1) > 0

        valid_attention_mask_to_find_min_k = replacing_attention_mask[permitted_to_replaceing]
        if len(valid_attention_mask_to_find_min_k) == 0:
            input_ids = input_ids - epsilon
            input_ids = input_ids.long()
            return input_ids, replacing_labels
        min_k = min((valid_attention_mask_to_find_min_k == 0).count_nonzero(dim=1))

        replacing_attention_mask[(replacing_attention_mask == 0).cumsum(dim=1) > min_k.item()] = 1

        replacing_input_ids = input_ids * (1 - replacing_attention_mask)
        fixed_input_ids = input_ids * replacing_attention_mask

        perm = torch.randperm(len(valid_attention_mask_to_find_min_k))

        if self.label_replacing:
            replacing_labels = labels.clone()
            permitted_to_replacing_labels = replacing_labels[permitted_to_replaceing]
            shuffeld_replacing_labels = permitted_to_replacing_labels[perm]
            replacing_labels[permitted_to_replaceing] = shuffeld_replacing_labels

        non_zero_replacing_input_ids = replacing_input_ids[replacing_input_ids != 0].view(len(valid_attention_mask_to_find_min_k), -1)
        shuffled_replacing_input_ids = non_zero_replacing_input_ids[perm]

        input_ids[fixed_input_ids == 0] = shuffled_replacing_input_ids.flatten()
        input_ids = input_ids - epsilon
        input_ids = input_ids.long()
        return input_ids, replacing_labels


    def _split_data(self, tensor, replacing_attention_mask):
        permitted_mask = torch.sum(replacing_attention_mask == 1, dim=1) > self.k
        fixed_tensor = tensor[~permitted_mask]
        flexible_tensor = tensor[permitted_mask]
        return flexible_tensor, fixed_tensor


    def execute(self, input_ids, mask, attention_mask, labels, debug=False, groups=None):
        mask = self.fix_inputs_with_tokens_less_than_k(input_ids, attention_mask, mask, self.k)
        replacing_attention_mask = attention_mask * mask

        more_than_k_input_ids, less_than_k_input_ids = self._split_data(input_ids, replacing_attention_mask)
        more_than_k_labels, less_than_k_labels = self._split_data(labels, replacing_attention_mask)
        more_than_k_replacing_attention_mask, less_than_k_replacing_attention_mask = self._split_data(replacing_attention_mask, replacing_attention_mask)

        replaced_input_ids, replacing_labels = self._replace(more_than_k_input_ids, more_than_k_replacing_attention_mask, more_than_k_labels)
        less_than_k_replaced_input_ids, less_than_k_replacing_labels = self._replace(less_than_k_input_ids, less_than_k_replacing_attention_mask, less_than_k_labels)

        replaced_input_ids = torch.cat([less_than_k_replaced_input_ids, replaced_input_ids])
        replacing_labels = torch.cat([less_than_k_replacing_labels, replacing_labels])

        if self.augmentation:
            augmentation_indices = replacing_labels != labels

            augmentation_input_ids = input_ids[augmentation_indices]
            augmentation_replaced_input_ids = replaced_input_ids[augmentation_indices]
            replaced_input_ids = torch.cat([input_ids, augmentation_replaced_input_ids])
            input_ids = torch.cat([input_ids, augmentation_input_ids])


            augmentation_labels = labels[augmentation_indices]
            augmentation_replacing_labels = replacing_labels[augmentation_indices]
            replacing_labels = torch.cat([labels, augmentation_replacing_labels])
            labels = torch.cat([labels, augmentation_labels])

            augmentation_mask = mask[augmentation_indices]
            mask = torch.cat([mask, augmentation_mask])

            augmentation_attention_mask = attention_mask[augmentation_indices]
            attention_mask = torch.cat([attention_mask, augmentation_attention_mask])

        if debug:
            if self.augmentation:
                augmentation_groups = groups[augmentation_indices.cpu()]
                groups = torch.cat([groups, augmentation_groups])

            input_ids = torch.cat([less_than_k_input_ids, more_than_k_input_ids])
            labels = torch.cat([less_than_k_labels, more_than_k_labels])

            visualize(input_ids, replaced_input_ids, groups, labels, replacing_labels)

        return replaced_input_ids, mask, replacing_labels, attention_mask
```

<!-- #region id="wvkUeq4mfwWP" -->
# Model
<!-- #endregion -->

<!-- #region id="2k5S_ej-S_ZE" -->
## Grad-CAM
<!-- #endregion -->

```{python executionInfo={'elapsed': 19, 'status': 'ok', 'timestamp': 1693916820109, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="c75-BvEOf3sP"}
def compute_grad_cam(model, input_ids, attention_mask, token_type_ids=None,
                     position_ids=None, head_mask=None, inputs_embeds=None):

    copied_model = copy.deepcopy(model).to(DEVICE)

    outputs = copied_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
                            position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)
    logits = outputs['logits']

    # Backpropagate to get the gradients
    target_class = torch.argmax(logits, dim=1)
    one_hot = torch.zeros_like(logits).scatter(1, target_class.unsqueeze(1), 1.0)
    logits.backward(gradient=one_hot, retain_graph=True)

    # Get the gradients for each token in the input text
    # token_ids = inputs['input_ids']
    gradients = copied_model.bert_model.embeddings.word_embeddings.weight.grad[input_ids] #shape = [number of text, number of tokens, 768]
    gradients = torch.mean(gradients, dim=2)  # Aggregate gradients across layers   shape = [number of text, number of tokens]
    gradients = abs(gradients)
    return gradients
```

<!-- #region id="MWlsnvuiTCHK" -->
## My Bert
<!-- #endregion -->

```{python executionInfo={'elapsed': 19, 'status': 'ok', 'timestamp': 1693916820110, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="UxMXOWaGDNyG"}
class MyBert(nn.Module):
    def __init__(self, bert_model, num_labels, k,
                 num_layers=1, d_model=MAX_LENGTH,
                 num_heads=1, dff=2048,
                 max_length=MAX_LENGTH):
        super(MyBert, self).__init__()

        self.num_labels = num_labels
        self.k = k

        #Base Model
        self.bert_model = bert_model

        #Transformer Attention
        self.attention_mask_predictor = TransformerRationalePredictor(
            num_layers=num_layers,
            d_model=d_model,
            num_heads=num_heads,
            dim_feedforward=dff,
            )
        self.attention_mask_predictor.to(DEVICE)
        self.copied_model = None

    def _cal_continuity_loss(self, z):
        return torch.mean(torch.abs(z[:, 1:] - z[:, :-1]))

    def get_mask(self, predicted_attention_mask, k, use_grad_cam):
        if use_grad_cam:
            z = predicted_attention_mask
        else:
            z = torch.nn.functional.softmax(predicted_attention_mask, -1)
        indices = torch.topk(z[:, :], k=k).indices
        mask = torch.zeros([z.shape[0], z.shape[1]]).to(DEVICE)
        mask.scatter_(1, indices, 1.)
        with torch.no_grad():
            neg = mask-z[:,:]
        ret = neg + z[:,:]
        return ret

    def forward(self, input_ids, attention_mask=None, token_type_ids=None,
                position_ids=None, head_mask=None, inputs_embeds=None,
                labels=None, use_grad_cam=False,
                rational_replacing=False, rational_augmentation=False,
                train_agument=False, train_label_replacing=False,
                test_mode=False, test_reverse=False, debug=False, groups=None):



        assert not (test_mode and rational_replacing), print("Error! You can not use rational_replacing while test_mode")
        assert not (test_mode and rational_augmentation), print("Error! You can not use rational_augmentation while test_mode")
        assert not (test_mode and train_agument), print("Error! You can not use train_agument while test_mode")
        assert not (test_mode and test_reverse), print("Error! You can not use reverse while test_mode")

        assert not (rational_replacing and test_reverse), print("Error! You can not use both rational_replacing and reverse")
        assert not (rational_replacing and rational_augmentation), print("Error! You can not use both rational_replacing and rational_augmentation")

        assert not (rational_augmentation and test_reverse), print("Error! You can not use both rational_augmentation and reverse")
        assert not (rational_augmentation and test_reverse), print("Error! You can not use both rational_augmentation and train_agument")

        mask_loss = None
        mask = attention_mask.clone()

        if not test_mode:
            if use_grad_cam:
                predicted_attention_mask = (
                    compute_grad_cam(input_ids, attention_mask)
                    )
            else:
                predicted_attention_mask = (
                    self.attention_mask_predictor(input_ids.to(torch.float64))
                )
            predicted_attention_mask = torch.randn(predicted_attention_mask.shape).to(DEVICE)
            
            mask = self.get_mask(predicted_attention_mask, self.k, use_grad_cam=use_grad_cam)
            mask[:, 0] = 1
            mask[:, -1] = 1

        if test_reverse:
            method = Reverse(f'Reverseing Predicted Attention Mask')
            input_ids, mask, reversed_attention_mask = method.execute(input_ids, mask, attention_mask, predicted_attention_mask)
            mask = reversed_attention_mask

        if rational_replacing:
            method = RationalTopTokenReplacing(f'Rational Top Token Replacing{" with augmentation" if train_agument else ""}',
                                               k=self.k, use_grad_cam=use_grad_cam, augmentation=train_agument, label_replacing=train_label_replacing)
            input_ids, mask, labels, replacing_attention_mask = method.execute(input_ids, mask, attention_mask, labels, debug=debug, groups=groups)

            mask_loss = self._cal_continuity_loss(mask)
            mask = replacing_attention_mask

        outputs = self.bert_model(input_ids, attention_mask=mask, token_type_ids=token_type_ids,
                                  position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds,
                                  labels=labels)
        logits = outputs['logits']

        if rational_augmentation:
            method = RationalAugmentation('Rational Augmentation', k=self.k, use_grad_cam=use_grad_cam)
            input_ids, mask, augmentation_attention_mask = method.execute(input_ids, mask, attention_mask, labels, logits.argmax(dim=1))
            outputs = self.bert_model(input_ids, attention_mask=augmentation_attention_mask, token_type_ids=token_type_ids,
                                      position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds,
                                      labels=labels)
            logits = outputs['logits']

        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            if mask_loss:
                weight = [1, 0]
                loss = (
                    weight[0] * loss_fn(logits.view(-1, self.num_labels), labels.view(-1)) +
                    weight[1] * mask_loss
                ) / sum(weight)
            else:
                loss = loss_fn(logits.view(-1, self.num_labels), labels.view(-1))
            return {'loss': loss, 'logits': logits, 'labels': labels}
        else:
            return {'logits': logits}
```

<!-- #region id="Rks-RoIMu738" -->
# Training Stuff


<!-- #endregion -->

```{python executionInfo={'elapsed': 929, 'status': 'ok', 'timestamp': 1693916821023, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="7xlkNMXASuSZ"}
args = {
    'bert_tuning': {
        'num_epochs': 5,
        'lr': 2e-5,
        # 'lr': 0.001,
        'batch_size': 32,
    },
    'bert_last_layer_tuning': {
        'num_epochs': 10,
        'lr': 0.001,
        'batch_size': 2048,
    },
    'rational_transformer_training': {
        'num_epochs': 2,
        'lr': 2e-5,
        'batch_size': 128,
    },
    'my_bert_tuning': {
        'num_epochs': 2,
        'lr': 2e-5,
        'batch_size': 32,
    },
    'my_bert_grad_cam_tuning': {
        'num_epochs': 1,
        'lr': 0.0001,
        'batch_size': 128,
    },
    'my_bert_prime_tuning': {
        'num_epochs': 5,
        'lr': 0.00001,
        'batch_size': 32,
    },
}
```

<!-- #region id="HTHoQW7kCP7e" -->
## Functions
<!-- #endregion -->

```{python executionInfo={'elapsed': 22, 'status': 'ok', 'timestamp': 1693916821025, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="r6lhmkMkTaOs"}
def run_epoch(run_type, model, data_loader, optimizer, scheduler=None, **kwargs):
    assert run_type in ['Train', 'Test', 'Val'], print("Error! undefined run_epoch type")
    if 'debug' not in kwargs:
        kwargs['debug'] = False

    pbar = tqdm(data_loader)

    accuracy = 0
    epoch_loss = 0
    if run_type in ['Test', 'Val']:
        model.eval()
    else:
        model.train()
        optimizer.zero_grad()

    all_preds = []
    all_labels = []
    all_groups = []

    with torch.set_grad_enabled(run_type == 'Train'):
        for batch_idx, batch in enumerate(pbar):
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            labels = batch['labels'].to(DEVICE)
            groups = batch['groups']
            segments_ids = batch['segments_ids'].to(DEVICE)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels,
                            token_type_ids=segments_ids,
                            use_grad_cam=kwargs['use_grad_cam'],
                            rational_replacing=kwargs['rational_replacing'],
                            rational_augmentation=kwargs['rational_augmentation'],
                            train_label_replacing=kwargs['label_replacing'],
                            train_agument=kwargs['agument'],
                            test_reverse = kwargs['test_reverse'],
                            test_mode = kwargs['test_not_masking'],
                            debug=kwargs['debug'])

            loss = outputs['loss']
            logits = outputs['logits']
            preds = torch.argmax(logits, dim=1)
            if 'labels' in outputs:
                labels = outputs['labels']

            if run_type == 'Train':
                if DATA_SET_TYPE == 'MultiNLI':
                    max_grad_norm = 1.0
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                    optimizer.step()
                    scheduler.step()
                    # model.zero_grad()
                else:
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

            epoch_loss += loss.item()

            labels = labels.cpu().numpy()
            preds = preds.cpu().numpy()
            accuracy += accuracy_score(labels, preds)

            avg_loss = epoch_loss / ((batch_idx + 1))
            avg_accuracy = accuracy / ((batch_idx + 1))
            pbar.set_description(f"{run_type} => AvgLoss:{avg_loss:.4f}, AvgAcc:{avg_accuracy:.4f}")

            all_preds.extend(preds)
            all_labels.extend(labels)
            all_groups.extend(groups.numpy())

    if run_type == 'Test':
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        all_groups = np.array(all_groups)

        for group in np.unique(all_groups):
            mask = all_groups == group
            group_labels = all_labels[mask]
            group_preds = all_preds[mask]
            precision, recall, f1, _ = precision_recall_fscore_support(group_labels, group_preds, average='micro')
            print(f"group={group} ==> acc={precision} - count={sum(mask)}")

    data_loader_len = len(data_loader)
    epoch_loss /= data_loader_len
    accuracy /= data_loader_len

    return model, loss, accuracy
```

```{python executionInfo={'elapsed': 20, 'status': 'ok', 'timestamp': 1693916821026, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="lct0XAz1ol5O"}
def save_model(model, model_name, epoch, lr, batch_size, k=None):
    if k:
        model_path = f"./models/dataset={DATA_SET_TYPE}/max_length={MAX_LENGTH}/{model_name}/epoch={epoch}_lr={lr}_batch_size={batch_size}_k={k}.pt"
    else:
        model_path = f"./models/dataset={DATA_SET_TYPE}/max_length={MAX_LENGTH}/{model_name}/epoch={epoch}_lr={lr}_batch_size={batch_size}.pt"

    if os.path.exists(model_path):
        print("WARNING: Model already exist!, nothing saved")
        return None

    if not os.path.exists(os.path.dirname(model_path)):
        os.makedirs(os.path.dirname(model_path))

    torch.save(model.state_dict(), model_path)
    print("model saved in this path:")
    print(model_path)
```

```{python executionInfo={'elapsed': 20, 'status': 'ok', 'timestamp': 1693916821028, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="YoQ7Tveju739"}
def fine_tune(model, train_args, test_args,
              num_epochs=NUM_EPOCHS, lr=LEARNING_RATE, batch_size=BATCH_SIZE,
              train_dataset=train_dataset, test_dataset=test_dataset,
              is_save_model=False, model_name=None, k=None):

    # Define data loader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)
    if DATA_SET_TYPE == 'MultiNLI':
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

    # Define the optimizer and the loss function
    if DATA_SET_TYPE != 'MultiNLI':
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
        scheduler = None
    else:
        adam_epsilon = 1e-6
        warmup_steps = 0
        warmup_percent = 0.2

        no_decay = ["bias", "LayerNorm.weight"]
        optimizer_grouped_parameters = [
            {
                "params": [
                    p for n, p in model.named_parameters()
                    if not any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
            {
                "params": [
                    p for n, p in model.named_parameters()
                    if any(nd in n for nd in no_decay)
                ],
                "weight_decay": 0.0,
            },
        ]
        t_total = math.ceil(num_epochs*len(train_dataset)*1./batch_size)
        warmup_steps = int(t_total*warmup_percent)
        optimizer = AdamW(model.parameters(), lr=lr, eps=adam_epsilon, correct_bias=False)
        scheduler = get_linear_schedule_with_warmup(optimizer,
                                                    num_warmup_steps=warmup_steps,
                                                    num_training_steps=t_total)
    # Define the training loop
    for epoch in range(num_epochs):

        model, train_loss, train_acc = run_epoch("Train", model, train_loader, optimizer, scheduler, **train_args)
        _, test_loss, test_acc = run_epoch("Test", model, test_loader, optimizer, scheduler, **test_args)

        print(f'Epoch {epoch + 1}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')
        
        save_model(model, model_name, epoch, lr, batch_size, k=k)

    return model
```

```{python executionInfo={'elapsed': 20, 'status': 'ok', 'timestamp': 1693916821029, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="GBDsGZmpqz9J"}
def evaluate_model(model, eval_args, dataset=test_dataset, batch_size=BATCH_SIZE):

    test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    _, test_loss, test_acc = run_epoch("Test", model, test_loader, optimizer=None, scheduler=None, **eval_args)

    print(f'eval_loss={test_loss:.4f}, eval_acc={test_acc:.4f}')
```

<!-- #region id="rajYNmLzsRgt" jp-MarkdownHeadingCollapsed=true -->
# IMDB dataset Training
<!-- #endregion -->

<!-- #region id="jvipo7VQEGW8" jp-MarkdownHeadingCollapsed=true -->
## Normal IMDB Dataset
<!-- #endregion -->

```{python id="TIlSsqC3ELeb"}
bert_model = Bert(num_labels=2, tune_only_last_layer=False)
bert_model = bert_model.to(DEVICE)
fine_tuned_bert_model = fine_tune(bert_model, *args['bert_tuning'].values())
```

<!-- #region id="Xn978RlAeOPL" jp-MarkdownHeadingCollapsed=true -->
## Random Spurious IMDB Dataset (My Bert)
<!-- #endregion -->

<!-- #region id="UzC6lPebgMJ6" jp-MarkdownHeadingCollapsed=true -->
### Training Bert on Random Spurious IMDB
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 424}, executionInfo={'elapsed': 557360, 'status': 'ok', 'timestamp': 1690125094627, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="qCR2YJXjtTst", outputId="a1049c92-f152-4fb1-e603-8e82c93c7aed"}
bert_model = Bert(num_labels=2, tune_only_last_layer=False)
bert_model = bert_model.to(DEVICE)
fine_tuned_bert_model = fine_tune(bert_model, *args['bert_tuning'].values())
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 1498, 'status': 'ok', 'timestamp': 1690125605483, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="VGCARcP3S24k", outputId="147faa20-22bf-493b-c8e5-ae17f94e2e53"}
save_model(model=fine_tuned_bert_model, model_name='fine_tuned_bert_model',
           epoch=args['bert_tuning']['num_epochs'], lr=args['bert_tuning']['lr'],
           batch_size=args['bert_tuning']['batch_size'])
```

<!-- #region id="2SaYPMn0gW_8" jp-MarkdownHeadingCollapsed=true -->
### Training Rational Transformer On Random Sputious IMDB
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 6212, 'status': 'ok', 'timestamp': 1690647298387, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="EYfI3FlM6wP1", outputId="db832f5d-6544-48f1-ecb4-e194ac0f0d91"}
# fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
# fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
# fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=RandomSpuriousImdb/max_length=64/fine_tuned_bert/epoch=5_lr=0.0001_batch_size=64.pt'))
```

```{python id="VoPUWsxNW6Zg"}
for name, param in fine_tuned_bert_model.named_parameters():
    param.requires_grad = False
k = 10

rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 180}, executionInfo={'elapsed': 165619, 'status': 'ok', 'timestamp': 1690644612587, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="uL25XYvYQBqE", outputId="6a0ad463-550d-4fc9-8d10-081ecd3d4943"}
fine_tuned_rational_transformer_model = fine_tune(rational_transformer_model, *args['rational_transformer_training'].values(), test_reverse=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 1753, 'status': 'ok', 'timestamp': 1690127199390, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="90XLrSCbXCyU", outputId="0c281dec-4e99-49e3-c5aa-c15f6ca57ef8"}
save_model(model=fine_tuned_rational_transformer_model, model_name='fine_tuned_rational_transformer_model',
           epoch=args['rational_transformer_training']['num_epochs'], lr=args['rational_transformer_training']['lr'],
           batch_size=args['rational_transformer_training']['batch_size'], k=k)
```

<!-- #region id="fnYUxFPAYtx3" jp-MarkdownHeadingCollapsed=true -->
### Fine-tuning My Bert on Random Spurious Dataset (Replacing)
<!-- #endregion -->

```{python id="2ZF65Mr5Y1mh"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=RandomSpuriousImdb/max_length=64/fine_tuned_bert/epoch=5_lr=0.0001_batch_size=64.pt'))
k = 10
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=RandomSpuriousImdb/max_length=64/fine_tuned_rational_transformer_model/epoch=2_lr=1e-06_batch_size=128_k=10.pt'))
```

```{python id="9gsOAQBpY4G7"}
for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 180}, executionInfo={'elapsed': 223343, 'status': 'ok', 'timestamp': 1690648769933, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="IswE3lsLRb7a", outputId="6bf97a79-85c8-4e6f-a0a5-64def544688a"}
my_bert_model = fine_tune(fine_tuned_rational_transformer_model, *args['my_bert_tuning'].values(), train_replace=True, test_not_masking=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 1739, 'status': 'ok', 'timestamp': 1690127947743, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="rd4kKgZ2cPce", outputId="ce7a02d1-5e92-4aa2-886d-b8e8746c35d7"}
save_model(model=my_bert_model, model_name='my_bert_model',
           epoch=args['my_bert_tuning']['num_epochs'], lr=args['my_bert_tuning']['lr'],
           batch_size=args['my_bert_tuning']['batch_size'], k=k)
```

<!-- #region id="6drXD0Lrfg8F" jp-MarkdownHeadingCollapsed=true -->
### Fine-tuning My Bert on Random Spurious Dataset (Miss Classified Agumentation)
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 8346, 'status': 'ok', 'timestamp': 1690649278393, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="9SiPDj2pfmOo", outputId="5d0b6297-76af-43aa-954b-1fee96b39c85"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=RandomSpuriousImdb/max_length=64/fine_tuned_bert/epoch=5_lr=0.0001_batch_size=64.pt'))
k = 10
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=RandomSpuriousImdb/max_length=64/fine_tuned_rational_transformer_model/epoch=2_lr=1e-06_batch_size=128_k=10.pt'))
```

```{python id="SKEINoKZfpQB"}
for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 180}, executionInfo={'elapsed': 294192, 'status': 'ok', 'timestamp': 1690649572544, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="CdwHMQjzfraz", outputId="95d05b40-a77a-45f6-9e82-5a9accc3caba"}
my_bert_model = fine_tune(fine_tuned_rational_transformer_model, *args['my_bert_tuning'].values(), train_agument=True, test_not_masking=True)
```

<!-- #region id="SnC3pK1NgAz7" jp-MarkdownHeadingCollapsed=true -->
## Random Spurious IMDB Dataset (My Bert Prime)
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 2181, 'status': 'ok', 'timestamp': 1690290409690, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="f6UY0y8OpgMP", outputId="051dae47-42b4-4303-a045-1bc5aa9cd94c"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=RandomSpuriousImdb/max_length=64/fine_tuned_bert/epoch=5_lr=0.0001_batch_size=64.pt'))
```

```{python id="s_m39xPYgGqj"}
k = 20

my_bert_prime_model = MyBertPrime(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)

for name, param in my_bert_prime_model.named_parameters():
    if 'classifier' in name or 'bert_model.pooler' in name or 'attention_mask_predictor' in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 49}, id="CeIStfGjJ85t", outputId="bdb76a76-af17-4ae8-b341-6e36e95eac65"}
fine_tuned_my_bert_prime_model = fine_tune(my_bert_prime_model, *args['my_bert_prime_tuning'].values(), test_not_masking=True)
```

```{python colab={'background_save': True, 'base_uri': 'https://localhost:8080/', 'height': 685}, executionInfo={'elapsed': 394489, 'status': 'error', 'timestamp': 1690290338892, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="QRpX2Dc5Eu4A", outputId="d0379010-a1df-40ae-ccd5-0732aff1fd81"}
fine_tuned_my_bert_prime_model = fine_tune(my_bert_prime_model, *args['my_bert_prime_tuning'].values(), test_not_masking=True)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 424}, executionInfo={'elapsed': 442526, 'status': 'ok', 'timestamp': 1690286913716, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="uLGpMyIn6Ynv", outputId="c25829b5-af46-400e-eec6-0f60c57014db"}
fine_tuned_my_bert_prime_model = fine_tune(my_bert_prime_model, *args['my_bert_prime_tuning'].values(), test_not_masking=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 1439, 'status': 'ok', 'timestamp': 1690287069586, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="YLorHjWf8shU", outputId="4860e87b-2d1c-4061-8ff5-0fee5ec60f53"}
save_model(model=fine_tuned_my_bert_prime_model, model_name='fine_tuned_my_bert_prime_model',
           epoch=args['my_bert_prime_tuning']['num_epochs'], lr=args['my_bert_prime_tuning']['lr'],
           batch_size=args['my_bert_prime_tuning']['batch_size'], k=k)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 261}, executionInfo={'elapsed': 264390, 'status': 'ok', 'timestamp': 1690285979676, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="56NxuqkLxRsU", outputId="6e25538e-5e79-49e9-ce8a-481c27e4c0d1"}
fine_tuned_my_bert_prime_model = fine_tune(my_bert_prime_model, *args['my_bert_prime_tuning'].values(), test_not_masking=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 619, 'status': 'ok', 'timestamp': 1690286244040, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="xjLIMgJ15xo7", outputId="9a17f4a4-0fac-4f3c-c7ea-9082717534f3"}
save_model(model=fine_tuned_my_bert_prime_model, model_name='fine_tuned_my_bert_prime_model',
           epoch=args['my_bert_prime_tuning']['num_epochs'], lr=args['my_bert_prime_tuning']['lr'],
           batch_size=args['my_bert_prime_tuning']['batch_size'], k=k)
```

<!-- #region id="fzbnwoXn9n89" jp-MarkdownHeadingCollapsed=true -->
## Random Spurious IMDB Dataset (X-Grad Cam Tuning)
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 7509, 'status': 'ok', 'timestamp': 1690645038640, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="kDYy35ti9tr7", outputId="a22e69e7-96ad-4cc2-c9dc-93e5cef0182e"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=RandomSpuriousImdb/max_length=64/fine_tuned_bert/epoch=5_lr=0.0001_batch_size=64.pt'))
```

```{python id="335C1h6FneDt"}
k = 10
my_bert_grad_cam_model = MyBert(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 551533, 'status': 'ok', 'timestamp': 1690561688995, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="EPu4PrL6SZYn", outputId="f2090c9c-394f-44f5-e24e-79660eb9b1bc"}
fine_tuned_my_bert_grad_cam_model = fine_tune(my_bert_grad_cam_model, *args['my_bert_grad_cam_tuning'].values(), test_not_masking=True, use_grad_cam=True, train_replace=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 4061, 'status': 'ok', 'timestamp': 1690561769898, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="5dtmySCWVSfk", outputId="c15dab31-bd1c-4edd-b7b0-eb42cacc442e"}
save_model(model=fine_tuned_my_bert_grad_cam_model, model_name='fine_tuned_my_bert_grad_cam_model',
           epoch=args['my_bert_grad_cam_tuning']['num_epochs'], lr=args['my_bert_grad_cam_tuning']['lr'],
           batch_size=args['my_bert_grad_cam_tuning']['batch_size'])
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 179497, 'status': 'ok', 'timestamp': 1690645219631, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="VPbyZpCYS-16", outputId="4a28fca8-4ba3-4ad5-dc8f-be7554b9e104"}
fine_tuned_my_bert_grad_cam_model = fine_tune(my_bert_grad_cam_model, *args['my_bert_grad_cam_tuning'].values(), test_not_masking=True, use_grad_cam=True, train_replace=True)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 552467, 'status': 'ok', 'timestamp': 1690560837038, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="sEbzcP8OPrfX", outputId="bb1927f9-701f-48e4-b623-0a06805a5019"}
fine_tuned_my_bert_grad_cam_model = fine_tune(my_bert_grad_cam_model, *args['my_bert_grad_cam_tuning'].values(), test_not_masking=True, use_grad_cam=True, train_replace=True)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 554392, 'status': 'ok', 'timestamp': 1690557874946, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="yjpDb6vDRrLH", outputId="f3fceea5-6c29-408f-f651-3d83555fea8b"}
fine_tuned_my_bert_grad_cam_model = fine_tune(my_bert_grad_cam_model, *args['my_bert_grad_cam_tuning'].values(), test_not_masking=True, use_grad_cam=True, train_replace=True)
```

```{python id="oavnLWunSZUE"}

```

```{python id="slAN7_swSZP1"}

```

```{python id="GwQEXxK4tJWZ"}
import matplotlib.pyplot as plt
def plot_grad_cam(tokens, gradients):
    # Plot the heatmap
    plt.figure(figsize=(14, 6))
    text = tokenizer.decode(tokens)
    if 'hamid' in text:
        print('hamid')
    if 'akbar' in text:
        print('akbar')
    plt.bar(range(len(tokens)), gradients.cpu(), align='center', tick_label=[tokenizer.decode(int(token)) for token in tokens])
    plt.xlabel('Tokens')
    plt.ylabel('Gradient Importance')
    plt.title('Grad-CAM Importance for each Token')
    plt.xticks(rotation=90, ha='right')
    plt.tight_layout()
    plt.show()
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 49}, executionInfo={'elapsed': 9922, 'status': 'ok', 'timestamp': 1690643748969, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="KFL52Dnn-5QQ", outputId="fd169bc4-15e7-49b2-d32d-3bb21d1d28fd"}
test_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)
model = my_bert_grad_cam_model
model.eval()
test_loss = 0
test_acc = 0
test_preds = []
test_labels = []
pbar = tqdm(test_loader)
for batch_idx, batch in enumerate(pbar):
    input_ids = batch['input_ids'].to(DEVICE)
    attention_mask = batch['attention_mask'].to(DEVICE)
    labels = batch['labels'].to(DEVICE)

    outputs = model(input_ids, attention_mask=attention_mask, labels=labels, use_grad_cam=True, debug=True)

    break
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 12, 'status': 'ok', 'timestamp': 1690643755952, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="SMFNVwMyN2xV", outputId="027ee916-4d15-455e-8b8e-f9317a78d63e"}
labels
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 1000}, executionInfo={'elapsed': 16605, 'status': 'ok', 'timestamp': 1690386894315, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="dbUrCA-T-5K2", outputId="7ccabe24-272d-4b43-a12b-53882899f0ba"}
import matplotlib.pyplot as plt
model = fine_tuned_bert_model
def compute_grad_cam(input_ids, attention_mask, token_type_ids=None,
                     position_ids=None, head_mask=None, inputs_embeds=None):
        # inputs = inputs.to(DEVICE)
        # if not self.copied_model:
            # self.copied_model = copy.deepcopy(self.bert_model).to(DEVICE)
        copied_model = copy.deepcopy(model).to(DEVICE)

        # for name, param in copied_model.named_parameters():
        #     param.requires_grad = True

        outputs = copied_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
                               position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)
        logits = outputs['logits']

        # Backpropagate to get the gradients
        target_class = torch.argmax(logits, dim=1)
        one_hot = torch.zeros_like(logits).scatter(1, target_class.unsqueeze(1), 1.0)
        logits.backward(gradient=one_hot, retain_graph=True)

        # Get the gradients for each token in the input text
        # token_ids = inputs['input_ids']
        gradients = copied_model.bert_model.embeddings.word_embeddings.weight.grad[input_ids] #shape = [number of text, number of tokens, 768]
        gradients = torch.mean(gradients, dim=2)  # Aggregate gradients across layers   shape = [number of text, number of tokens]

        return gradients

# Example usage
# input_text = ["This movie hamid was not great", "This movie was not great"]
# inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True)
# inputs = batch
gradients = compute_grad_cam(input_ids, attention_mask)
for input, grad in zip(input_ids, gradients):
    plot_grad_cam(input, grad)

```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 1000}, executionInfo={'elapsed': 13490, 'status': 'ok', 'timestamp': 1690387009369, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="X1EPyfMd5zD8", outputId="1db4034d-71d2-4a8b-f151-a3258ced988f"}
gradients = compute_grad_cam(input_ids, attention_mask)
gradients = abs(gradients)
for input, grad in zip(input_ids, gradients):
    plot_grad_cam(input, grad)

```

<!-- #region id="xPUeAnoXJPvp" jp-MarkdownHeadingCollapsed=true -->
## Chunk Spurious IMDB Dataset (My Bert)
<!-- #endregion -->

<!-- #region id="Zy2dlqA8J0Gk" jp-MarkdownHeadingCollapsed=true -->
### Training Bert on Chunk Spurious IMDB
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 424}, executionInfo={'elapsed': 1607695, 'status': 'ok', 'timestamp': 1690577225907, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="lJOxspDtJ0Gm", outputId="7a68c249-9f21-447e-8b10-0fa01e7b2e38"}
bert_model = Bert(num_labels=2, tune_only_last_layer=False)
bert_model = bert_model.to(DEVICE)
fine_tuned_bert_model = fine_tune(bert_model, *args['bert_tuning'].values())
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 2491, 'status': 'ok', 'timestamp': 1690577228362, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="gLZGOisrJ0Gn", outputId="56b566db-7075-4a82-d31d-7eb3b03c9843"}
save_model(model=fine_tuned_bert_model, model_name='fine_tuned_bert_model',
           epoch=args['bert_tuning']['num_epochs'], lr=args['bert_tuning']['lr'],
           batch_size=args['bert_tuning']['batch_size'])
```

<!-- #region id="JBeDXWCJJ0Go" jp-MarkdownHeadingCollapsed=true -->
### Training Rational Transformer On Chunk Sputious IMDB
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 2945, 'status': 'ok', 'timestamp': 1690542666165, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="rwH5w7DQJ0Gp", outputId="41bbfab0-eafe-4e8d-e4da-c65bd027adb4"}
# fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
# fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
# fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=ChunkSpuriousImdb/max_length=64/fine_tuned_bert_model/epoch=5_lr=0.0001_batch_size=64.pt'))
```

```{python id="FL1wgobZJ0Gq"}
for name, param in fine_tuned_bert_model.named_parameters():
    param.requires_grad = False
k = 10

rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 180}, executionInfo={'elapsed': 470323, 'status': 'ok', 'timestamp': 1690577817303, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="c5Z-cuWcQjtt", outputId="c2e535ca-dca3-4655-8981-1d8575ab2038"}
fine_tuned_rational_transformer_model = fine_tune(rational_transformer_model, *args['rational_transformer_training'].values(), test_reverse=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 2250, 'status': 'ok', 'timestamp': 1690577834465, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="R-Rn5V4BQlS7", outputId="9a3dc23e-a82e-4cea-9741-09bb8c2e40b9"}
save_model(model=fine_tuned_rational_transformer_model, model_name='fine_tuned_rational_transformer_model',
           epoch=args['rational_transformer_training']['num_epochs'], lr=args['rational_transformer_training']['lr'],
           batch_size=args['rational_transformer_training']['batch_size'], k=k)
```

<!-- #region id="62r76UhGJ0Gs" jp-MarkdownHeadingCollapsed=true -->
### Fine-tuning My Bert on Chunk Spurious Dataset
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 4250, 'status': 'ok', 'timestamp': 1690578560942, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="eo2tHBNhJ0Gt", outputId="4161dd27-4a59-4571-f158-8b40bb7ec96a"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=ChunkSpuriousImdb/max_length=64/fine_tuned_bert_model/epoch=5_lr=0.0001_batch_size=64.pt'))
k = 10
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=ChunkSpuriousImdb/max_length=64/fine_tuned_rational_transformer_model/epoch=2_lr=1e-06_batch_size=128_k=10.pt'))
```

```{python id="JbOiComcJ0Gu"}
for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 180}, executionInfo={'elapsed': 635714, 'status': 'ok', 'timestamp': 1690578504306, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="o-oNaTnBQns8", outputId="670d6252-977f-4798-9f60-cc384881afac"}
my_bert_model = fine_tune(fine_tuned_rational_transformer_model, *args['my_bert_tuning'].values(), train_replace=True, test_not_masking=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 980, 'status': 'ok', 'timestamp': 1690578505282, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="67My2W1GQovD", outputId="a4a87b78-0e3b-4449-c17a-cd1c991daff4"}
save_model(model=my_bert_model, model_name='my_bert_model',
           epoch=args['my_bert_tuning']['num_epochs'], lr=args['my_bert_tuning']['lr'],
           batch_size=args['my_bert_tuning']['batch_size'], k=k)
```

<!-- #region id="_t_uRh0XVfOi" jp-MarkdownHeadingCollapsed=true -->
## Chunk Spurious IMDB Dataset (X-Grad Cam Tuning)
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 7225, 'status': 'ok', 'timestamp': 1690580935901, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="mjLbN_kpVfOu", outputId="8eaeabb8-55c9-437c-c153-e34c1345ba3a"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=ChunkSpuriousImdb/max_length=64/fine_tuned_bert_model/epoch=5_lr=0.0001_batch_size=64.pt'))
```

```{python id="UumJzpjcVfOv"}
k = 10
my_bert_grad_cam_model = MyBert(fine_tuned_bert_model, num_labels=2, k=k).to(DEVICE)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 538527, 'status': 'ok', 'timestamp': 1690579240669, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="B26C421zVfOv", outputId="b7e6a2d7-7251-4189-e343-71347db2670d"}
fine_tuned_my_bert_grad_cam_model = fine_tune(my_bert_grad_cam_model, *args['my_bert_grad_cam_tuning'].values(), test_not_masking=True, use_grad_cam=True, train_replace=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 6877, 'status': 'ok', 'timestamp': 1690579247519, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="O7WngQuIVfOv", outputId="f68b0fa5-961f-40f1-f840-47e6d395f311"}
save_model(model=fine_tuned_my_bert_grad_cam_model, model_name='fine_tuned_my_bert_grad_cam_model',
           epoch=args['my_bert_grad_cam_tuning']['num_epochs'], lr=args['my_bert_grad_cam_tuning']['lr'],
           batch_size=args['my_bert_grad_cam_tuning']['batch_size'])
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 542146, 'status': 'ok', 'timestamp': 1690580572191, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="HyXtZN0PVfOw", outputId="4959b649-2961-466e-a3db-7e00a952131c"}
fine_tuned_my_bert_grad_cam_model = fine_tune(my_bert_grad_cam_model, *args['my_bert_grad_cam_tuning'].values(), test_not_masking=True, use_grad_cam=True, train_replace=True)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 537731, 'status': 'ok', 'timestamp': 1690581474919, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="28WKKvZrVfOv", outputId="76e14f47-0e25-4591-dfc5-c7a33e762ca9"}
fine_tuned_my_bert_grad_cam_model = fine_tune(my_bert_grad_cam_model, *args['my_bert_grad_cam_tuning'].values(), test_not_masking=True, use_grad_cam=True, train_replace=True)
```

<!-- #region id="G5svWxkcsXc5" -->
#MultiNLI Training
<!-- #endregion -->

<!-- #region id="vDcvbiOiftyG" -->
# MultiNLI Dataset (My Bert)
<!-- #endregion -->

<!-- #region id="HNUCR8IZgEj4" jp-MarkdownHeadingCollapsed=true -->
### Training Bert on MultiNLI
<!-- #endregion -->

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
bert_model = bert_model.to(DEVICE)
fine_tuned_bert_model = fine_tune(bert_model, train_args, test_args, *args['bert_tuning'].values(), is_save_model=True, model_name='fine_tuned_bert_model_debugged')
```

<!-- #region id="zxyN4OGJdHdL" -->
### Training Rational Transformer On MultiNLI
<!-- #endregion -->

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': True,
    'test_not_masking': False,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))

for name, param in fine_tuned_bert_model.named_parameters():
    param.requires_grad = False
k = 7

rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k).to(DEVICE)
fine_tuned_rational_transformer_model = fine_tune(rational_transformer_model, 
                                                  train_args, test_args, *args['rational_transformer_training'].values(), 
                                                  is_save_model=True, k=k,
                                                  model_name="fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03")
```

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': True,
    'test_not_masking': False,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))

for name, param in fine_tuned_bert_model.named_parameters():
    param.requires_grad = False
k = 12

rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k).to(DEVICE)
fine_tuned_rational_transformer_model = fine_tune(rational_transformer_model, 
                                                  train_args, test_args, *args['rational_transformer_training'].values(), 
                                                  is_save_model=True, k=k,
                                                  model_name="fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03")
```

<!-- #region id="c4gXvz7K3r8n" jp-MarkdownHeadingCollapsed=true -->
### Fine-tuning My Bert on MultiNLI (Rational Repalcing)
<!-- #endregion -->

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 5
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=5.pt'))
fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_with_reinit_model_epoch=2_wga_erm=68.03")
```

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 5
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=5.pt'))
# fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_model_epoch=2_wga_erm=68.03")
```

-----------------------

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 7
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=7.pt'))
fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_with_reinit_model_epoch=2_wga_erm=68.03")
```

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 7
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=7.pt'))
# fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_model_epoch=2_wga_erm=68.03")
```

--------------------

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 12
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=12.pt'))
# fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_model_epoch=2_wga_erm=68.03")
```

<!-- #region jp-MarkdownHeadingCollapsed=true -->
### Fine-tuning My Bert on MultiNLI (Rational Repalcing with Label)
<!-- #endregion -->

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': True,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 5
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=5.pt'))
# fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_with_label_model_epoch=2_wga_erm=68.03")
```

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': True,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 7
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=7.pt'))
# fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_with_label_model_epoch=2_wga_erm=68.03")
```

```{python}
train_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': True,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}

test_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'label_replacing': False,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 12
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=12.pt'))
# fine_tuned_rational_transformer_model.bert_model.bert_model.classifier = torch.nn.Linear(fine_tuned_rational_transformer_model.bert_model.bert_model.config.hidden_size, 3)
fine_tuned_rational_transformer_model = fine_tuned_rational_transformer_model.to(DEVICE)

for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False

my_bert_model = fine_tune(fine_tuned_rational_transformer_model, train_args, test_args, *args['my_bert_tuning'].values(), 
                          is_save_model=True, k=k,
                          model_name="my_bert_tuning_with_label_model_epoch=2_wga_erm=68.03")
```

<!-- #region id="8RdbqszW0qY3" jp-MarkdownHeadingCollapsed=true -->
### Evaluations
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 376}, executionInfo={'elapsed': 1121133, 'status': 'ok', 'timestamp': 1692552956045, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="ruYwXJWk0t6U", outputId="1e9043c5-c3d5-4c1c-db6c-87023ad9fc3f"}
eval_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = Bert(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model/epoch=5_lr=2e-05_batch_size=32.pt'))

print("Fine-Tuned Bert Result:")
print("Train:")
evaluate_model(fine_tuned_bert_model, eval_args, dataset=train_dataset, batch_size=128)
print("Test:")
evaluate_model(fine_tuned_bert_model, eval_args, dataset=test_dataset, batch_size=128)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 205}, executionInfo={'elapsed': 431887, 'status': 'ok', 'timestamp': 1693514622445, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '17115744166251793158'}, 'user_tz': -210}, id="k7iVcT6JSCH0", outputId="b3051ab2-49e2-4298-9f2a-a82014f79684"}
eval_args = {
    'use_grad_cam': False,
    'rational_replacing': False,
    'rational_augmentation': False,
    'test_reverse': False,
    'test_not_masking': True,
}

fine_tuned_bert_model = Bert(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model/epoch=5_lr=2e-05_batch_size=32.pt'))
k = 7
my_bert_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k).to(DEVICE)
my_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/my_bert_model/epoch=2_lr=2e-05_batch_size=32_k=7.pt'))

print("My Bert (Rational Replacing) Result:")
print("Test:")
evaluate_model(my_bert_model, eval_args, dataset=test_dataset, batch_size=128,)
```

<!-- #region id="o02Cuke8Qyil" jp-MarkdownHeadingCollapsed=true -->
# Civil Comments Training
<!-- #endregion -->

<!-- #region id="uQAF5DYdmQVQ" -->
## JTT Civil Comments Dataset (My Bert)
<!-- #endregion -->

<!-- #region id="fe30alc5mQVR" jp-MarkdownHeadingCollapsed=true -->
### Training Bert on JTT Civil Comments
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 17, 'status': 'ok', 'timestamp': 1692357608316, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="wtB564ouX0f6", outputId="c3ea3c0c-b169-45a5-b873-f092db820def"}
bert_model = Bert(num_labels=2, tune_only_last_layer=False)
bert_model = bert_model.to(DEVICE)
fine_tuned_bert_model = fine_tune(bert_model, *args['bert_tuning'].values())
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 17, 'status': 'ok', 'timestamp': 1692357550329, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="Qj9GsjjaXmX8", outputId="bbec8aca-c424-43e2-8213-1be87739cb0e"}
save_model(model=bert_model, model_name='fine_tuned_bert_model',
           epoch=args['bert_tuning']['num_epochs'], lr=args['bert_tuning']['lr'],
           batch_size=args['bert_tuning']['batch_size'])
```

<!-- #region id="a-mzfZS3mQVW" jp-MarkdownHeadingCollapsed=true -->
### Training Rational Transformer On JTT Civil Comments
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 9, 'status': 'ok', 'timestamp': 1692358142579, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="Uf7z23SSZ985", outputId="0af746f2-40f4-4010-9906-9656579c9687"}
fine_tuned_bert_model
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 349}, executionInfo={'elapsed': 26, 'status': 'error', 'timestamp': 1692358000467, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="GU74X5ohmQVX", outputId="ee2256d7-6229-49ec-fedb-f6f5e0ef344a"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=JTTCivilComments/max_length=128/fine_tuned_bert_model/epoch=3_lr=2e-05_batch_size=32.pt'))
```

```{python id="8yCJivhwmQVY"}
for name, param in fine_tuned_bert_model.named_parameters():
    param.requires_grad = False
k = 7

rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k).to(DEVICE)
```

```{python colab={'background_save': True, 'base_uri': 'https://localhost:8080/', 'height': 237}, executionInfo={'elapsed': 1676421, 'status': 'ok', 'timestamp': 1691346001685, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="oB5OyL_-mQVY", outputId="ab8274b3-8585-4c1b-e8d8-2a0ef366713c"}
fine_tuned_rational_transformer_model = fine_tune(rational_transformer_model, *args['rational_transformer_training'].values(), test_reverse=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 5098, 'status': 'ok', 'timestamp': 1691346006779, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="7GsYcL5-mQVZ", outputId="c4eed745-19ef-4710-c8a6-a40eed548f72"}
save_model(model=fine_tuned_rational_transformer_model, model_name='fine_tuned_rational_transformer_model',
           epoch=args['rational_transformer_training']['num_epochs'], lr=args['rational_transformer_training']['lr'],
           batch_size=args['rational_transformer_training']['batch_size'], k=k)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 98}, executionInfo={'elapsed': 1735369, 'status': 'ok', 'timestamp': 1691258614496, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="Gv9Thw-PmQVa", outputId="4d0abf57-848a-47e5-c8d7-eee3fd0f72fa"}
fine_tuned_rational_transformer_model = fine_tune(rational_transformer_model, *args['rational_transformer_training'].values(), test_reverse=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 5742, 'status': 'ok', 'timestamp': 1691258620200, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="31NsPKlKmQVb", outputId="f7d9d5f5-6e41-4e55-994e-9c4c1ff95009"}
save_model(model=fine_tuned_rational_transformer_model, model_name='fine_tuned_rational_transformer_model',
           epoch=args['rational_transformer_training']['num_epochs'], lr=args['rational_transformer_training']['lr'],
           batch_size=args['rational_transformer_training']['batch_size'], k=k)
```

<!-- #region id="BhoO5xXFmQVc" jp-MarkdownHeadingCollapsed=true -->
### Fine-tuning My Bert on JTT Civil Comments
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 66}, executionInfo={'elapsed': 40206, 'status': 'ok', 'timestamp': 1691940750259, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="K7aacxT_mQVd", outputId="de70d52d-e87b-4ab3-d194-9d1461560f12"}
fine_tuned_bert_model = Bert(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=GDROMultiNLI/max_length=64/fine_tuned_bert_model/epoch=5_lr=2e-05_batch_size=32.pt'))
k = 7
fine_tuned_rational_transformer_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k).to(DEVICE)
fine_tuned_rational_transformer_model.load_state_dict(torch.load('./models/dataset=GDROMultiNLI/max_length=64/fine_tuned_rational_transformer_model/epoch=1_lr=1e-06_batch_size=128_k=7.pt'))
```

```{python id="B2e6rEHSmQVe"}
for name, param in fine_tuned_rational_transformer_model.named_parameters():
    # if 'classifier' in name:
    if 'attention_mask_predictor' not in name:
        param.requires_grad = True
    else:
        param.requires_grad = False
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 387}, executionInfo={'elapsed': 6730, 'status': 'error', 'timestamp': 1691940765181, 'user': {'displayName': 'HamidReza Yaghoubi', 'userId': '02734612338194496039'}, 'user_tz': -210}, id="TqRTitMjmQVf", outputId="36be08df-a2d8-4c99-aee6-725a74703ae8"}
my_bert_model = fine_tune(fine_tuned_rational_transformer_model, *args['my_bert_tuning'].values(), train_agument=True, test_not_masking=True)
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 180}, executionInfo={'elapsed': 9477523, 'status': 'ok', 'timestamp': 1691882570727, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="3MQX34t6mQVg", outputId="a9451bdb-1f4d-4c03-9aeb-313ffb1b89f9"}
my_bert_model = fine_tune(fine_tuned_rational_transformer_model, *args['my_bert_tuning'].values(), train_agument=True, test_not_masking=True)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 5541, 'status': 'ok', 'timestamp': 1691882576261, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="5d3UJkB3mQVh", outputId="60b65663-f0c5-4199-aa8c-55bc8a530b95"}
save_model(model=my_bert_model, model_name='my_bert_model_agumented',
           epoch=args['my_bert_tuning']['num_epochs'], lr=args['my_bert_tuning']['lr'],
           batch_size=args['my_bert_tuning']['batch_size'], k=k)
```

<!-- #region id="W2gIfrwKqi26" -->
# Visualization
<!-- #endregion -->

```{python}
labelsssss = []
```

```{python id="SydKhBkF1lP2"}
def visualize_model(model, visualization_args, dataset=train_dataset, batch_size=32, tokenizer=tokenizer):

    model.eval()
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    i = 0
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(data_loader)):
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            labels = batch['labels'].to(DEVICE)
            groups = batch['groups']

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels,
                            use_grad_cam=visualization_args['use_grad_cam'],
                            rational_replacing=visualization_args['rational_replacing'],
                            rational_augmentation=visualization_args['rational_augmentation'],
                            train_label_replacing=visualization_args['label_replacing'],
                            train_agument=visualization_args['agument'],
                            test_reverse = visualization_args['test_reverse'],
                            test_mode = visualization_args['test_not_masking'],
                            debug=True, groups=groups)
            print(torch.argmax(outputs['logits'], dim=1))
            labelsssss.extend(torch.argmax(outputs['logits'], dim=1).cpu().numpy())
            i +=1
            if i == 20:
                break
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 3163, 'status': 'ok', 'timestamp': 1693664566976, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="X2pkI7uKqyez", outputId="76fe8767-18a4-492f-c3ec-855dcb022aaf"}
fine_tuned_bert_model = BertForSequence(num_labels=3, tune_only_last_layer=False)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_bert_model_debugged/epoch=2_lr=2e-05_batch_size=32.pt'))
k = 12
my_bert_model = MyBert(fine_tuned_bert_model, num_labels=3, k=k).to(DEVICE)
my_bert_model.load_state_dict(torch.load('./models/dataset=MultiNLI/max_length=64/fine_tuned_rational_transformer_model_epoch=2_wga_erm=68.03/epoch=1_lr=2e-05_batch_size=128_k=12.pt'))
```

```{python colab={'background_save': True, 'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 120, 'status': 'ok', 'timestamp': 1693664566977, 'user': {'displayName': 'Hamidreza Yaghoubi', 'userId': '02901691238973721504'}, 'user_tz': -210}, id="2NPW9AN5CvIO", outputId="67f74bf9-bce4-440b-8d45-0e59b94bc2d6"}
visualization_args = {
    'use_grad_cam': False,
    'rational_replacing': True,
    'rational_augmentation': False,
    'label_replacing': True,
    'agument': False,
    'test_reverse': False,
    'test_not_masking': False,
}
visualize_model(my_bert_model, visualization_args)
```

```{python}
pd.DataFrame(data={'label': labelsssss}).value_counts()
```

```{python}
train_pdf[['label']].value_counts()
```

```{python}

```

```{python}

```

```{python}

```

```{python}
for random_index in np.random.randint(0, len(words['group']), 150):
    print(f"{Fore.BLUE}===============idx={random_index} - group={words['group'][random_index]}===============")
    print(" ".join(words['first_text'][random_index]))
    print(" ".join(words['second_text'][random_index]))
```

```{python}

```

```{python}
tmp = pd.DataFrame(data={
    'chosen': words['chosen'],
    'replaced': words['replaced'][:1009516],
    'group': [x.item() for x in words['group']],
})

tmp.groupby('group').agg('count')
```

```{python}
group_1 = tmp[tmp['group'] == 1].groupby('chosen').agg('count').reset_index().sort_values('replaced', ascending=False)
group_1[
    (group_1['chosen'] == 'nobody') |
    (group_1['chosen'] == 'no') |
    (group_1['chosen'] == 'never') |
    (group_1['chosen'] == 'nothing') 
]
```

```{python}
group_2 = tmp[tmp['group'] == 2].groupby('chosen').agg('count').reset_index().sort_values('replaced', ascending=False)
group_2[
    (group_2['chosen'] == 'nobody') |
    (group_2['chosen'] == 'no') |
    (group_2['chosen'] == 'never') |
    (group_2['chosen'] == 'nothing') 
]
```

```{python}
group_3 = tmp[tmp['group'] == 3].groupby('chosen').agg('count').reset_index().sort_values('replaced', ascending=False)
group_3[
    (group_3['chosen'] == 'nobody') |
    (group_3['chosen'] == 'no') |
    (group_3['chosen'] == 'never') |
    (group_3['chosen'] == 'nothing') 
]
```

```{python}
group_4 = tmp[tmp['group'] == 4].groupby('chosen').agg('count').reset_index().sort_values('replaced', ascending=False)
group_4[
    (group_4['chosen'] == 'nobody') |
    (group_4['chosen'] == 'no') |
    (group_4['chosen'] == 'never') |
    (group_4['chosen'] == 'nothing') 
]
```

```{python}
group_5 = tmp[tmp['group'] == 5].groupby('chosen').agg('count').reset_index().sort_values('replaced', ascending=False)
group_5[
    (group_5['chosen'] == 'nobody') |
    (group_5['chosen'] == 'no') |
    (group_5['chosen'] == 'never') |
    (group_5['chosen'] == 'nothing') 
]
```

```{python}

```

```{python}

```

```{python}

```

<!-- #region id="LT5HBk94r_yH" jp-MarkdownHeadingCollapsed=true -->
# Debug
<!-- #endregion -->

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 429}, executionInfo={'elapsed': 7211, 'status': 'error', 'timestamp': 1690285026726, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="RtxNsz2eon14", outputId="235620a7-2997-47ad-d64f-d7da719af9b3"}
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)
model = fine_tuned_bert_model
model.eval()
test_loss = 0
test_acc = 0
test_preds = []
test_labels = []
pbar = tqdm(test_loader)
for batch_idx, batch in enumerate(pbar):
    input_ids = batch['input_ids'].to(DEVICE)
    attention_mask = batch['attention_mask'].to(DEVICE)
    labels = batch['labels'].to(DEVICE)

    outputs = model(input_ids, attention_mask=attention_mask, labels=labels, not_masking=False)
    loss = outputs['loss']
    logits = outputs['logits']
    test_loss += loss.item()

    preds = torch.argmax(logits, dim=1)
    test_acc += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())
    avg_loss = test_loss / ((batch_idx + 1))
    avg_acc = test_acc / ((batch_idx + 1))

    pbar.set_description(f"AvgTestLoss: {avg_loss:.4f}, AvgTestAcc: {avg_acc:.4f}")

    test_preds.extend(preds.cpu().numpy())
    test_labels.extend(labels.cpu().numpy())

test_loss /= len(test_loader)
test_acc /= len(test_loader)
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 5299, 'status': 'ok', 'timestamp': 1688333778617, 'user': {'displayName': 'HamidReza Yaghoubi Araghi', 'userId': '16858751766525091369'}, 'user_tz': -210}, id="zNnAlvMgsDed", outputId="6c45a83d-21cd-47ab-f74f-68793ec0020e"}
fine_tuned_bert_model = Bert(num_labels=2, tune_only_last_layer=True)
fine_tuned_bert_model = fine_tuned_bert_model.to(DEVICE)
fine_tuned_bert_model.load_state_dict(torch.load('our_spurious_imdb_fine_tuned_bert_model_epoch=5_lr=0.0001_batch_size=64_max_length=64.pt'))

fine_tuned_my_bert_model = MyBert(fine_tuned_bert_model, num_labels=2, k=30, debug=True).to(DEVICE)
fine_tuned_my_bert_model.load_state_dict(torch.load('our_spurious_imdb_fine_tuned_my_bert_k=40_2-1_epoch=5_lr=0.0001_batch_size=64_max_length=64.pt'))
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 626, 'status': 'ok', 'timestamp': 1689781767660, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="SPPtz5fAtxhz", outputId="98dbfc3b-0e4b-4d7e-8057-c60b710d9f00"}
test_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
counter = 0
for batch in test_loader:
    input_ids = batch['input_ids'].to(DEVICE)
    attention_mask = batch['attention_mask'].to(DEVICE)
    labels = batch['labels'].to(DEVICE)
    output = fine_tuned_my_bert_model(input_ids, attention_mask=attention_mask, debug=True, replacing=False)
    print(labels[0])
    print(output['logits'][0])
    break
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 425, 'status': 'ok', 'timestamp': 1689782022837, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="q4Ro3maP1Rae", outputId="f9f30230-179f-41a1-fb1f-bf23bee3bbd8"}
import numpy as np
input =[  101,  2023,  2003,  2026,  5440,  3185,  2412,  1012,  1045,  2031,
         3427,  2009,  2012,  2560, 20730,  2184,  2335,  1998,  1045,  5390,
         2296,  2051,  1012,  2026,  2155, 27591,  2033,  2025,  2000,  3422,
         2009,  2061,  1045,  2180,  2102,  2031,  1037,  6933,  4906,  1012,
         1045,  2228,  1045,  2293,  2008,  2009,  2003,  1037,  2995,  2466,
         2517,  2011, 14405, 19291,  2063,  2370,  2074,  2004,  2172,  2004,
         1045,  2293,  1996,   102]

# new_input = [  101, 24811,  2890,  4143,  2126,  2067,  2012,  1996,  6440,  1997,
#          2529, 10585,  6814,  3549,  2071,  2038,  1998,  2081,  8982,  4022,
#          1010,  2718,  2169,  2060,  2058,  1996,  4641,  2007,  2054,  2412,
#          2027,  2071,  6723,  1010,  1998,  2308,  2020,  2196,  2464,  1998,
#          4593,  2062,  2028,  2051,  2018, 17448,  1012, 15732,  2003,  3549,
#          2973,  1999,  1037, 20969,  7578,  2555,  2007,  1037, 10338, 10610,
#          2080,  1010,  1037,   102]


string_list = tokenizer.decode(input).split()
# new_string_list = tokenizer.decode(new_input).split()

new_mask = [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,
        0., 1., 0., 1., 0., 0., 0., 0., 1., 1.]

masked_string_list = [string_list[i] if new_mask[i] else 'None' for i in range(len(string_list))]

filtered_string_list = [string_list[i] for i in range(len(string_list)) if new_mask[i] == 0]
# new_filtered_string_list = [new_string_list[i] for i in range(len(new_string_list)) if new_mask[i] == 0]
# filtered_string_list = [token for token in filtered_string_list if token != '[PAD]']

print('------ORIGINAL TEXT------')
print(' '.join(string_list))

print("-------MASKED TEXT-------")
print(' '.join(masked_string_list))

print("------MASKED TOKEN-------")
print(filtered_string_list)
# print(new_filtered_string_list)
print(len(filtered_string_list))

# print('------SHUFFLED TEXT------')
# print(' '.join(new_string_list))
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 35}, executionInfo={'elapsed': 431, 'status': 'ok', 'timestamp': 1689782065133, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="YZC2V3w32-IV", outputId="eb10bae2-902f-470b-bae0-339e6bd12cfa"}
new_mask[14]
string_list[14]
```

```{python colab={'base_uri': 'https://localhost:8080/', 'height': 35}, executionInfo={'elapsed': 17, 'status': 'ok', 'timestamp': 1689782167946, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="ZF0DSxqg0gI3", outputId="0ce88842-3cd2-42ec-a416-2c6edff341ca"}
max_token = np.argmax(
    [-1.4610e+00, -2.8886e-01, -1.0807e+00,  6.8984e-02,  5.2259e-01,
        -1.2650e-01, -4.1524e-01, -8.3243e-01,  2.7661e-01, -1.8780e-01,
        -2.7425e-01,  5.8449e-01, -7.4261e-01, -1.3977e+00,  3.1624e+00,
        -5.3190e-01, -4.2031e-01,  2.7610e-01,  1.8121e-01,  4.5520e-01,
        -7.7785e-03, -6.4468e-02, -1.5800e-01,  6.2348e-01, -5.2412e-01,
         4.6673e+00,  9.4550e-01,  1.8885e-01,  1.0058e+00,  9.6701e-02,
        -4.4912e-01, -1.3513e+00,  1.7839e-01, -1.4030e-01,  3.8562e-01,
        -7.6057e-01,  5.7889e-02,  1.0548e+00, -5.5306e-01, -2.4999e-01,
        -3.6421e-01,  8.3747e-03, -1.3832e+00, -6.0916e-01, -9.7640e-01,
        -5.9565e-01, -4.1884e-01, -1.2850e+00,  1.8451e-01, -5.2520e-01,
         2.2948e-03, -5.4675e-01,  1.4637e+00,  2.0995e+00, -8.5624e-02,
         1.2197e+00, -6.3237e-01,  7.6313e-01, -2.2039e-01, -5.0600e-01,
        -3.4569e-02, -1.5507e-01,  7.7819e-01, -8.9426e-01]
    )

tokenizer.decode([input[max_token]])
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 12, 'status': 'ok', 'timestamp': 1689782169473, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="acwTHTwuFlh_", outputId="9e51068b-1bda-49fa-cf53-1bee108086f9"}
max_token
```

```{python colab={'base_uri': 'https://localhost:8080/'}, executionInfo={'elapsed': 12, 'status': 'ok', 'timestamp': 1689705287790, 'user': {'displayName': 'Liên Hồng', 'userId': '16972839904354450080'}, 'user_tz': -210}, id="O5sbJw-3jAGO", outputId="709bfc59-33c1-481e-ceb8-febea4e9e8a9"}
tokenizer('hamid')
```
